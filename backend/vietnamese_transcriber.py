"""
Advanced Vietnamese Real-time Speech Transcriber
Sá»­ dá»¥ng Whisper vá»›i VAD, noise reduction vÃ  Vietnamese language model
"""
from __future__ import annotations

import os
import sys
import subprocess
import logging
import tempfile
import json
import asyncio
import threading
import queue
import time
from pathlib import Path
from typing import Optional, Dict, Any, List, Callable
import numpy as np
from dataclasses import dataclass

from concurrent.futures import ThreadPoolExecutor

# Setup logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('transcriber.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TranscriptionSegment:
    """Äáº¡i diá»‡n cho má»™t segment Ã¢m thanh Ä‘Æ°á»£c transcribe"""
    text: str
    confidence: float
    start_time: float
    end_time: float
    language_confidence: float
    is_vietnamese: bool

@dataclass
class TranscriptionConfig:
    """Cáº¥u hÃ¬nh cho transcriber"""
    chunk_duration: float = 3.0  # seconds
    overlap_duration: float = 0.5  # seconds
    min_confidence: float = 0.6
    use_vad: bool = True
    denoise_audio: bool = True
    language_detection: bool = True
    real_time_callback: Optional[Callable] = None
    max_workers: int = 2

class AudioProcessor:
    """Xá»­ lÃ½ audio vá»›i VAD vÃ  denoising"""
    
    def __init__(self):
        self.is_initialized = False
        self._initialize()
    
    def _initialize(self):
        """Khá»Ÿi táº¡o cÃ¡c thÆ° viá»‡n audio processing"""
        try:
            # CÃ i Ä‘áº·t dependencies náº¿u cáº§n
            self._install_audio_dependencies()
            
            import librosa
            import noisereduce as nr
            import webrtcvad
            
            self.librosa = librosa
            self.nr = nr
            self.vad = webrtcvad.Vad(3)  # Aggressiveness level 3
            self.is_initialized = True
            logger.info("âœ… Audio processor initialized")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize audio processor: {e}")
            self.is_initialized = False
    
    def _install_audio_dependencies(self):
        """CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n audio processing - chá»‰ cháº¡y náº¿u cáº§n"""
        packages = [
            "librosa>=0.10.0",
            "noisereduce>=3.0.0", 
            "webrtcvad>=2.0.10",
            "soundfile>=0.12.1",
            "scipy>=1.9.0"
        ]
        
        missing_packages = []
        
        # Check what's already installed
        for package in packages:
            try:
                module_name = package.split('>=')[0].replace('-', '_')
                __import__(module_name)
                logger.info(f"âœ… {module_name} already available")
            except ImportError:
                missing_packages.append(package)
        
        # Only install missing packages
        if missing_packages:
            logger.info(f"ğŸ“¦ Installing {len(missing_packages)} missing audio packages: {', '.join(missing_packages)}")
            for package in missing_packages:
                try:
                    subprocess.check_call([
                        sys.executable, "-m", "pip", "install", package, "--quiet"
                    ], timeout=300)
                    logger.info(f"âœ… {package} installed")
                except subprocess.TimeoutExpired:
                    logger.error(f"â° Timeout installing {package}")
                    raise
                except Exception as e:
                    logger.error(f"âŒ Failed to install {package}: {e}")
                    raise
        else:
            logger.info("âœ… All audio packages already available")
    
    def preprocess_audio(self, audio_path: str, config: TranscriptionConfig) -> str:
        """Tiá»n xá»­ lÃ½ audio vá»›i denoising vÃ  VAD"""
        try:
            if not self.is_initialized:
                return audio_path
            
            # Load audio
            y, sr = self.librosa.load(audio_path, sr=16000, mono=True)
            
            # Denoise if enabled
            if config.denoise_audio:
                try:
                    y = self.nr.reduce_noise(y=y, sr=sr, prop_decrease=0.6, time_mask_smooth_ms=32)
                    logger.info("ğŸ”‡ Audio denoised")
                except Exception as de:
                    logger.warning(f"âš ï¸ Denoise failed: {de}")
            
            # Voice Activity Detection
            if config.use_vad:
                try:
                    y = self._apply_vad(y, sr)
                    logger.info("ğŸ¤ VAD applied")
                except Exception as ve:
                    logger.warning(f"âš ï¸ VAD failed: {ve}")
            
            # Normalize audio
            y = self._normalize_audio(y)
            
            # Save processed audio
            import soundfile as sf
            processed_path = audio_path.replace('.', '_processed.')
            sf.write(processed_path, y, sr)
            
            return processed_path
            
        except Exception as e:
            logger.warning(f"âš ï¸ Audio preprocessing failed: {e}")
            return audio_path
    
    def _apply_vad(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """Ãp dá»¥ng Voice Activity Detection vá»›i timeout"""
        try:
            import threading
            import time
            
            result = [audio]  # Default fallback
            exception = [None]
            
            def vad_worker():
                try:
                    # Convert to 16-bit PCM for webrtcvad
                    audio_16bit = (audio * 32767).astype(np.int16).tobytes()
                    
                    # Frame duration in ms
                    frame_duration = 30  # ms
                    frame_length = int(sr * frame_duration / 1000)
                    
                    # Process frames
                    voiced_frames = []
                    frame_count = 0
                    max_frames = min(1000, len(audio_16bit) // (frame_length * 2))  # Limit frames
                    
                    for i in range(0, len(audio_16bit), frame_length * 2):
                        if frame_count >= max_frames:
                            break
                            
                        frame = audio_16bit[i:i + frame_length * 2]
                        if len(frame) == frame_length * 2:
                            is_speech = self.vad.is_speech(frame, sr)
                            if is_speech:
                                start_sample = i // 2
                                end_sample = start_sample + frame_length
                                voiced_frames.append(audio[start_sample:end_sample])
                        frame_count += 1
                    
                    if voiced_frames:
                        result[0] = np.concatenate(voiced_frames)
                    else:
                        result[0] = audio  # Use original if no speech detected
                        
                except Exception as e:
                    exception[0] = e
            
            # Run VAD with timeout
            thread = threading.Thread(target=vad_worker)
            thread.daemon = True
            thread.start()
            thread.join(timeout=10)  # 10 second timeout
            
            if thread.is_alive():
                logger.warning("âš ï¸ VAD timeout, using original audio")
                return audio
            elif exception[0]:
                logger.warning(f"âš ï¸ VAD failed: {exception[0]}")
                return audio
            else:
                return result[0]
                
        except Exception as e:
            logger.warning(f"âš ï¸ VAD failed: {e}")
            return audio
    
    def _normalize_audio(self, audio: np.ndarray) -> np.ndarray:
        """Chuáº©n hÃ³a audio"""
        # Remove DC offset
        audio = audio - np.mean(audio)
        
        # Normalize to [-1, 1]
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.9
        
        return audio

class VietnameseLanguageModel:
    """Vietnamese language model Ä‘á»ƒ cáº£i thiá»‡n accuracy"""
    
    def __init__(self):
        self.vietnamese_words = set()
        self.no_accent_words = set()
        self.no_accent_map = {}
        self.common_phrases = {}
        self.name_entities = set()
        self.correction_rules = {}
        self.hotwords = set()
        self.context_corrector = None
        self._load_language_resources()
    
    def _load_language_resources(self):
        """Load Vietnamese language resources"""
        try:
            # Load tá»« Ä‘iá»ƒn
            tudien_path = Path(__file__).parent.parent / "tudien"
            
            vocab_files = {
                "tudien.txt": "general",
                "tudien-khongdau.txt": "no_accent", 
                "danhtu.txt": "nouns",
                "dongtu.txt": "verbs",
                "tinhtu.txt": "adjectives",
                "tenrieng.txt": "names"
            }
            
            for filename, category in vocab_files.items():
                file_path = tudien_path / filename
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        words = [w.strip().lower() for w in f.readlines() if w.strip()]
                        
                        if category == "names":
                            self.name_entities.update(words)
                        elif category == "no_accent":
                            self.no_accent_words.update(words)
                        else:
                            self.vietnamese_words.update(words)
                            # Build no-accent map for restoration
                            for w in words:
                                key = self._strip_accents(w)
                                if key:
                                    self.no_accent_map.setdefault(key, set()).add(w)
            
            # Load any extra *.txt wordlists in tudien as hotwords
            if tudien_path.exists():
                for extra_file in tudien_path.glob('*.txt'):
                    if extra_file.name in vocab_files:
                        continue
                    try:
                        with open(extra_file, 'r', encoding='utf-8') as f:
                            for line in f:
                                word = line.strip().lower()
                                if not word:
                                    continue
                                self.hotwords.add(word)
                                self.vietnamese_words.add(word)
                                key = self._strip_accents(word)
                                if key:
                                    self.no_accent_map.setdefault(key, set()).add(word)
                    except Exception:
                        pass
            
            # Load common Vietnamese phrases
            self._load_common_phrases()
            
            # Load correction rules
            self._load_correction_rules()
            
            logger.info(f"âœ… Loaded {len(self.vietnamese_words)} Vietnamese words")
            logger.info(f"âœ… Loaded {len(self.common_phrases)} common phrases")
            logger.info(f"âœ… Loaded {len(self.no_accent_words)} no-accent words; {len(self.no_accent_map)} accent groups")
            if self.hotwords:
                logger.info(f"âœ… Loaded {len(self.hotwords)} hotwords from tudien/*")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load language resources: {e}")
            self._load_fallback_vocabulary()
    
    def _load_common_phrases(self):
        """Load cÃ¡c cá»¥m tá»« phá»• biáº¿n tiáº¿ng Viá»‡t"""
        self.common_phrases = {
            # Greetings
            "xin chÃ o": "xin chÃ o",
            "chÃ o báº¡n": "chÃ o báº¡n", 
            "cáº£m Æ¡n": "cáº£m Æ¡n",
            "xin lá»—i": "xin lá»—i",
            
            # Common expressions
            "khÃ´ng sao": "khÃ´ng sao",
            "Ä‘Æ°á»£c rá»“i": "Ä‘Æ°á»£c rá»“i",
            "táº¥t nhiÃªn": "táº¥t nhiÃªn",
            "cÃ³ thá»ƒ": "cÃ³ thá»ƒ",
            "khÃ´ng thá»ƒ": "khÃ´ng thá»ƒ",
            
            # Questions
            "bao nhiÃªu": "bao nhiÃªu",
            "á»Ÿ Ä‘Ã¢u": "á»Ÿ Ä‘Ã¢u",
            "nhÆ° tháº¿ nÃ o": "nhÆ° tháº¿ nÃ o",
            "táº¡i sao": "táº¡i sao",
            "khi nÃ o": "khi nÃ o",
            
            # Time expressions
            "hÃ´m nay": "hÃ´m nay",
            "ngÃ y mai": "ngÃ y mai", 
            "hÃ´m qua": "hÃ´m qua",
            "tuáº§n trÆ°á»›c": "tuáº§n trÆ°á»›c",
            "thÃ¡ng tá»›i": "thÃ¡ng tá»›i",
        }
    
    
    def _load_correction_rules(self):
        """Load cÃ¡c quy táº¯c sá»­a lá»—i phá»• biáº¿n"""
        self.correction_rules = {
            # Common OCR-like errors in speech recognition
            "toi": "tÃ´i",
            "ban": "báº¡n",
            "duoc": "Ä‘Æ°á»£c",
            "khong": "khÃ´ng",
            "mot": "má»™t",
            "hai": "hai",
            "ba": "ba",
            "bon": "bá»‘n", 
            "nam": "nÄƒm",
            "sau": "sÃ¡u",
            "bay": "bÃ¢y",
            "tam": "tÃ¡m",
            "chin": "chÃ­n",
            "muoi": "mÆ°á»i",
            
            # Common speech recognition errors
            "xe á»Ÿ": "xin chÃ o",
            "kem on": "cáº£m Æ¡n",
            "sin loi": "xin lá»—i",
            "vang a": "vÃ¢ng áº¡",
            "kong a": "khÃ´ng áº¡",
        }

    @staticmethod
    def _strip_accents(text: str) -> str:
        try:
            import unicodedata
            return ''.join(c for c in unicodedata.normalize('NFD', text)
                           if unicodedata.category(c) != 'Mn')
        except Exception:
            return text
    
    def _load_fallback_vocabulary(self):
        """Load vocabulary dá»± phÃ²ng náº¿u khÃ´ng cÃ³ file tá»« Ä‘iá»ƒn"""
        self.vietnamese_words = {
            "tÃ´i", "báº¡n", "anh", "chá»‹", "em", "Ã´ng", "bÃ ", "con", "chÃ¡u",
            "lÃ ", "cÃ³", "khÃ´ng", "Ä‘Æ°á»£c", "ráº¥t", "nhiá»u", "Ã­t", "lá»›n", "nhá»",
            "Ä‘áº¹p", "xáº¥u", "tá»‘t", "xáº¥u", "nhanh", "cháº­m", "cao", "tháº¥p",
            "xin", "chÃ o", "cáº£m", "Æ¡n", "lá»—i", "vÃ¢ng", "khÃ´ng", "Ä‘Æ°á»£c",
            "nhÃ ", "trÆ°á»ng", "cÃ´ng", "ty", "bá»‡nh", "viá»‡n", "siÃªu", "thá»‹",
            "Äƒn", "uá»‘ng", "ngá»§", "lÃ m", "viá»‡c", "há»c", "Ä‘i", "vá»",
            "hÃ´m", "nay", "mai", "qua", "tuáº§n", "thÃ¡ng", "nÄƒm",
            "má»™t", "hai", "ba", "bá»‘n", "nÄƒm", "sÃ¡u", "báº£y", "tÃ¡m", "chÃ­n", "mÆ°á»i"
        }
    
    def correct_transcript(self, text: str, question: str = None) -> str:
        """Sá»­a lá»—i transcript dá»±a trÃªn language model vÃ  context"""
        try:
            if not text:
                return text
            
            # Apply traditional correction (context corrector removed)
            corrected = text.lower()
            
            # Apply phrase corrections first
            for wrong, correct in self.common_phrases.items():
                corrected = corrected.replace(wrong, correct)
            
            # Apply word-level corrections
            for wrong, correct in self.correction_rules.items():
                corrected = corrected.replace(wrong, correct)
            
            # Join spelled letters into words if they form a known term (e.g., "p h u c")
            corrected = self._join_spelled_letters(corrected)

            # Fuzzy-correct OOV tokens to nearest dictionary/hotword
            corrected = self._apply_fuzzy_dictionary_bias(corrected)

            # Capitalize names and proper nouns
            words = corrected.split()
            corrected_words = []
            
            for word in words:
                if word in self.name_entities:
                    corrected_words.append(word.capitalize())
                else:
                    corrected_words.append(word)
            
            result = " ".join(corrected_words)
            
            # Capitalize first letter
            if result:
                result = result[0].upper() + result[1:]
            
            return result
            
        except Exception as e:
            # DISABLED: Text correction error logging
            return text

    def _join_spelled_letters(self, text: str) -> str:
        tokens = text.split()
        output: List[str] = []
        i = 0
        while i < len(tokens):
            token = tokens[i]
            if len(token) == 1 and token.isalpha():
                # collect run of single letters
                j = i
                letters = []
                while j < len(tokens) and len(tokens[j]) == 1 and tokens[j].isalpha():
                    letters.append(tokens[j])
                    j += 1
                # Only join runs of >=2 letters
                if len(letters) >= 2:
                    joined = ''.join(letters).lower()
                    restored = self._restore_accents_if_possible(joined)
                    output.append(restored)
                    i = j
                    continue
            output.append(token)
            i += 1
        return ' '.join(output)

    def _restore_accents_if_possible(self, token: str) -> str:
        # If token exists as no-accent word or has accent variants, choose a known variant
        if token in self.no_accent_words and token in self.no_accent_map:
            # Prefer the most frequent or shortest variant; here choose first sorted
            candidates = sorted(self.no_accent_map[token], key=len)
            return candidates[0] if candidates else token
        # Fallback: try map even if token not listed in no_accent_words
        if token in self.no_accent_map:
            candidates = sorted(self.no_accent_map[token], key=len)
            return candidates[0] if candidates else token
        return token

    def _apply_fuzzy_dictionary_bias(self, text: str) -> str:
        import difflib
        words = text.split()
        corrected_words: List[str] = []
        vocab = self.vietnamese_words or set()
        # Merge hotwords to vocab
        if self.hotwords:
            vocab = set(vocab) | set(self.hotwords)
        for w in words:
            if w in vocab or len(w) <= 2 or any(ch.isdigit() for ch in w):
                corrected_words.append(w)
                continue
            # Try no-accent matching
            key = self._strip_accents(w)
            if key in self.no_accent_map:
                candidates = list(self.no_accent_map[key])
                # choose closest by difflib ratio on stripped
                best = max(candidates, key=lambda c: difflib.SequenceMatcher(None, key, self._strip_accents(c)).ratio())
                corrected_words.append(best)
                continue
            # Fallback: approximate match in vocab
            try:
                candidate = difflib.get_close_matches(w, vocab, n=1, cutoff=0.9)
                if candidate:
                    corrected_words.append(candidate[0])
                else:
                    corrected_words.append(w)
            except Exception:
                corrected_words.append(w)
        return ' '.join(corrected_words)
    
    def calculate_vietnamese_confidence(self, text: str) -> float:
        """TÃ­nh confidence cho Vietnamese text"""
        try:
            if not text:
                return 0.0
            
            words = text.lower().split()
            if not words:
                return 0.0
            
            # Count Vietnamese words
            vietnamese_count = sum(1 for word in words if word in self.vietnamese_words)
            
            # Count common phrases
            text_lower = text.lower()
            phrase_bonus = sum(0.2 for phrase in self.common_phrases.keys() 
                             if phrase in text_lower)
            
            base_confidence = vietnamese_count / len(words)
            final_confidence = min(base_confidence + phrase_bonus, 1.0)
            
            return round(final_confidence, 3)
            
        except Exception as e:
            logger.warning(f"âš ï¸ Confidence calculation failed: {e}")
            return 0.5

class RealTimeVietnameseTranscriber:
    """Real-time Vietnamese Speech Transcriber vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao"""
    
    def __init__(self, config: TranscriptionConfig = None):
        self.config = config or TranscriptionConfig()
        self.base_model_name = "gemini-2.5-flash"
        self.model = None
        self.processor = None
        self.is_initialized = False
        
        # Components
        self.audio_processor = AudioProcessor()
        self.language_model = VietnameseLanguageModel()
        
        # Real-time processing
        self.audio_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.is_processing = False
        self.executor = ThreadPoolExecutor(max_workers=self.config.max_workers)
        
        # Segments tracking
        self.processed_segments = []
        self.current_transcript = ""
        
        self._initialize_model()
    
    def _initialize_model(self):
        """Gemini API does not require local model init."""
        self.is_initialized = True
        logger.info("âœ… Vietnamese transcriber initialized (Gemini API)")
    
    def _install_dependencies(self):
        """No heavy deps needed for Gemini path; keep noop."""
        packages = []
        missing_packages = []
        
        # Check what's already installed
        for package in packages:
            try:
                module_name = package.split('>=')[0]
                if module_name == "torch":
                    import torch
                    logger.info("âœ… torch already available")
                elif module_name == "transformers":
                    import transformers
                    logger.info("âœ… transformers already available")
                elif module_name == "torchaudio":
                    import torchaudio
                    logger.info("âœ… torchaudio already available")
            except ImportError:
                missing_packages.append(package)
        
        # Only install missing packages
        if missing_packages:
            logger.info(f"ğŸ“¦ Installing {len(missing_packages)} missing packages: {', '.join(missing_packages)}")
            for package in missing_packages:
                try:
                    subprocess.check_call([
                        sys.executable, "-m", "pip", "install", package, "--quiet"
                    ], timeout=300)
                    logger.info(f"âœ… {package} installed")
                except subprocess.TimeoutExpired:
                    logger.error(f"â° Timeout installing {package}")
                    raise
                except Exception as e:
                    logger.error(f"âŒ Failed to install {package}: {e}")
                    raise
        else:
            logger.info("âœ… All required packages already available")
    
    def transcribe_audio_file(self, audio_path: str, language: str = 'vi', use_vietnamese_asr: bool = False, question: str = None) -> Dict[str, Any]:
        """Transcribe má»™t file audio (Gemini-first)."""
        try:
            if not os.path.exists(audio_path):
                return self._error_result(f"Audio file not found: {audio_path}")
            
            logger.info(f"ğŸµ Transcribing with Gemini (Google AI): {audio_path}")
            
            # Check file size
            file_size = os.path.getsize(audio_path)
            logger.info(f"ğŸ“ File size: {file_size / 1024:.1f} KB")
            
            # Check Gemini API key
            gemini_api_key = os.getenv('GEMINI_API_KEY')
            if not gemini_api_key:
                return self._error_result("Gemini API key not configured")

            # ASR option removed; always use Gemini model
            transcription_model = os.getenv('GEMINI_STT_MODEL', 'gemini-2.5-flash')
            logger.info(f"ğŸ¤ Using Gemini model: {transcription_model}")
            
            # Use Gemini API for transcription
            try:
                import google.generativeai as genai
                import base64
                
                genai.configure(api_key=gemini_api_key)
                model_name = transcription_model
                model = genai.GenerativeModel(model_name)
                
                # Compute duration (best-effort)
                try:
                    import soundfile as sf
                    f = sf.SoundFile(audio_path)
                    duration = len(f) / f.samplerate
                except Exception:
                    duration = 0.0
                
                logger.info("ğŸš€ Starting Gemini transcription...")
                start_time = time.time()
                
                # Prefer file upload API for robustness
                gemini_file = genai.upload_file(path=audio_path, mime_type="audio/wav")
                # Language-specific prompts for Gemini
                if language == 'vi':
                    logger.info("ğŸ‡»ğŸ‡³ Using enhanced Vietnamese-focused prompt for Gemini")
                    # Enhanced Vietnamese-focused prompt for Gemini
                    prompt = f"""
HÃ£y chÃ©p láº¡i CHÃNH XÃC ná»™i dung tiáº¿ng Viá»‡t trong audio nÃ y vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t:

ğŸ¯ YÃŠU Cáº¦U Äáº¶C BIá»†T CHO TIáº¾NG VIá»†T:
- ChÃº Ã½ Ä‘áº·c biá»‡t Ä‘áº¿n cÃ¡c tá»« cÃ³ dáº¥u: Ã¡, Ã , áº£, Ã£, áº¡, Ã©, Ã¨, áº», áº½, áº¹, Ã­, Ã¬, á»‰, Ä©, á»‹, Ã³, Ã², á», Ãµ, á», Ãº, Ã¹, á»§, Å©, á»¥, Ã½, á»³, á»·, á»¹, á»µ
- ChÃº Ã½ cÃ¡c phá»¥ Ã¢m Ä‘áº·c biá»‡t: Ä‘, nh, ng, ph, th, tr, ch, kh, gh, qu
- ChÃº Ã½ cÃ¡c tá»« cÃ³ thá»ƒ bá»‹ nháº§m láº«n: "tÃ´i" (khÃ´ng pháº£i "toi"), "báº¡n" (khÃ´ng pháº£i "ban"), "Ä‘Æ°á»£c" (khÃ´ng pháº£i "duoc")
- ChÃº Ã½ cÃ¡c tÃªn riÃªng Viá»‡t Nam: Nguyá»…n, Tráº§n, LÃª, Pháº¡m, HoÃ ng, VÅ©, VÃµ, Äáº·ng, BÃ¹i, Äá»—, Há»“, NgÃ´, DÆ°Æ¡ng, LÃ½

ğŸ” HÆ¯á»šNG DáºªN CHI TIáº¾T:
1. Láº¯ng nghe ká»¹ tá»«ng Ã¢m tiáº¿t vÃ  tá»«
2. PhÃ¢n biá»‡t rÃµ cÃ¡c thanh Ä‘iá»‡u: ngang, huyá»n, há»i, ngÃ£, náº·ng, sáº¯c
3. ChÃº Ã½ ngá»¯ cáº£nh Ä‘á»ƒ hiá»ƒu Ä‘Ãºng tá»« Ä‘Æ°á»£c nÃ³i
4. Náº¿u khÃ´ng cháº¯c cháº¯n, hÃ£y ghi láº¡i Ã¢m thanh gáº§n nháº¥t
5. Giá»¯ nguyÃªn cáº¥u trÃºc cÃ¢u vÃ  Ã½ nghÄ©a

ğŸ“ Äá»ŠNH Dáº NG Káº¾T QUáº¢:
- Chá»‰ tráº£ vá» transcript thuáº§n vÄƒn báº£n
- KhÃ´ng thÃªm dáº¥u cÃ¢u náº¿u khÃ´ng cháº¯c cháº¯n
- KhÃ´ng thÃªm tá»« hoáº·c cÃ¢u khÃ´ng cÃ³ trong audio
- Viáº¿t hoa Ä‘áº§u cÃ¢u náº¿u cáº§n thiáº¿t

HÃ£y báº¯t Ä‘áº§u chÃ©p láº¡i ná»™i dung audio:
"""
                else:
                    logger.info(f"ğŸŒ Using standard prompt for {language.upper()} language")
                    # English or other languages
                    prompt = f"""
Please transcribe the audio content accurately in {language.upper()} language.

ğŸ¯ REQUIREMENTS:
- Listen carefully to each word and syllable
- Pay attention to pronunciation and context
- If uncertain, write the closest sound you hear
- Maintain sentence structure and meaning

ğŸ“ OUTPUT FORMAT:
- Return only plain text transcript
- Don't add punctuation unless certain
- Don't add words or sentences not in the audio
- Capitalize first letter of sentences if needed

Please start transcribing the audio content:
"""
                response = model.generate_content([gemini_file, prompt])
                
                gemini_time = time.time()
                gemini_processing_time = gemini_time - start_time
                # Backward-compatible field name expected downstream
                whisper_processing_time = gemini_processing_time
                logger.info(f"âœ… Gemini transcription completed in {gemini_processing_time:.2f}s")
                
                # Extract text
                text = (getattr(response, 'text', None) or "").strip()
                confidence = 0.8  # Gemini does not provide confidence; set a reasonable default
                
                # Use GPT-4o to improve transcript quality - configurable via ENV
                improved_text = text
                gpt4o_processing_time = 0
                enable_gpt_improve = os.getenv('ENABLE_GPT_TRANSCRIPT_IMPROVE', 'false').lower() == 'true'
                
                # Check if text is meaningful (not just noise or silence)
                text_stripped = text.strip()
                
                # Moderately strict validation - only block explicit noise tokens
                meaningless_patterns = [
                    'silence', 'noise', 'background', 'static', 'hum', 'buzz'
                ]

                # Only block very specific marketing/marketing-related words
                suspicious_words = [
                    'ghiÃ©n mÃ¬ gÃµ', 'bá» lá»¡ video háº¥p dáº«n', 'subscribe ngay',
                    'Ä‘Äƒng kÃ½ kÃªnh Ä‘á»ƒ khÃ´ng bá» lá»¡', 'hÃ£y subscribe cho kÃªnh',
                    'youtube channel', 'facebook page', 'instagram account',
                    'tiktok creator', 'social media platform'
                ]
                
                # Check if text contains any suspicious words
                text_lower = text_stripped.lower()
                has_suspicious_content = any(word in text_lower for word in suspicious_words)
                
                # Only block very specific marketing phrases
                generic_phrases = [
                    'hÃ£y subscribe cho kÃªnh ghiá»n mÃ¬ gÃµ Ä‘á»ƒ khÃ´ng bá» lá»¡ nhá»¯ng video háº¥p dáº«n',
                    'subscribe ngay Ä‘á»ƒ khÃ´ng bá» lá»¡ video háº¥p dáº«n',
                    'Ä‘Äƒng kÃ½ kÃªnh Ä‘á»ƒ xem nhá»¯ng video háº¥p dáº«n',
                    'háº¹n gáº·p láº¡i cÃ¡c báº¡n trong nhá»¯ng video tiáº¿p theo',
                    'nhá»¯ng video tiáº¿p theo'
                ]

                has_generic_phrases = any(phrase in text_lower for phrase in generic_phrases)

                # Treat clearly hallucinated/marketing-like content as invalid speech
                meaningless_hit = any(pattern in text_lower for pattern in meaningless_patterns)
                is_suspicious = False
                # Only treat as no speech for explicit marketing phrases/words or common YT closing
                if has_suspicious_content or has_generic_phrases or ('háº¹n gáº·p láº¡i' in text_lower and 'video' in text_lower):
                    logger.warning("âš ï¸ Marketing-like transcript detected; treating as no speech")
                    text = ""
                    text_stripped = ""
                    improved_text = ""
                    confidence = 0.0
                    is_suspicious = True
                
                # Relaxed validation - enable GPT-4o for reasonable cases
                is_meaningful = True  # default before ENV gate

                # Only disable GPT-4o for very specific problematic cases
                if (len(text_stripped) < 3 or  # Too short
                    confidence < 0.1 or  # Very low confidence
                    any(pattern in text_stripped.lower() for pattern in meaningless_patterns) or
                    has_suspicious_content or
                    has_generic_phrases):
                    is_meaningful = False
                # ENV gate: disable GPT improvement unless explicitly enabled
                if not enable_gpt_improve:
                    is_meaningful = False
                
                used_gpt = False
                if is_meaningful:
                    logger.info("ğŸ¤– Improving transcript with GPT-4o...")
                    improved_text = self._improve_with_gpt4o(text, language)
                    used_gpt = True
                    
                    gpt4o_time = time.time()
                    gpt4o_processing_time = gpt4o_time - whisper_time
                    
                    # Validate that GPT-4o didn't add too much content
                    original_words = len(text.split())
                    improved_words = len(improved_text.split())
                    
                    if improved_words > original_words * 1.5:  # If GPT added more than 50% content
                        logger.warning(f"âš ï¸ GPT-4o added too much content: {original_words} -> {improved_words} words")
                        improved_text = text  # Revert to original
                        gpt4o_processing_time = 0
                        used_gpt = False
                    else:
                        logger.info(f"âœ¨ GPT-4o improvement completed in {gpt4o_processing_time:.2f}s")
                else:
                    logger.info(f"â­ï¸ Skipping GPT-4o improvement (text: '{text_stripped}', length: {len(text_stripped)}, confidence: {confidence:.2f})")
                
                total_processing_time = time.time() - start_time
                logger.info(f"â±ï¸ Total processing time: {total_processing_time:.2f}s")
                
                # Apply comprehensive language model corrections based on language
                if language == 'vi':
                    # Enhanced Vietnamese processing with multiple correction layers
                    logger.info("ğŸ¯ Applying comprehensive Vietnamese language processing...")

                    # Step 1: Basic language model correction
                    corrected_text = self.language_model.correct_transcript(improved_text, question)
                    vietnamese_confidence = self.language_model.calculate_vietnamese_confidence(corrected_text)

                    # Step 2: Apply comprehensive Vietnamese-specific corrections
                    corrected_text = self._apply_comprehensive_vietnamese_corrections(corrected_text, question)

                    # Step 3: Calculate final confidence after corrections
                    vietnamese_confidence = max(vietnamese_confidence, 0.85)  # Boost confidence for Vietnamese

                    # DISABLED: Vietnamese processing completion logging
                    # DISABLED: Final corrected text logging

                else:
                    # Enhanced English processing
                    # DISABLED: English processing logging
                    corrected_text = improved_text
                    vietnamese_confidence = confidence

                    # Apply basic English corrections
                    corrected_text = self._apply_english_specific_corrections(corrected_text)

                    # DISABLED: English processing completion logging

                # If transcript is empty after processing, use original Whisper text if available
                if not corrected_text or corrected_text.strip() == "":
                    if text and text.strip():
                        # DISABLED: Using original Whisper text logging
                        corrected_text = text.strip()
                        # Keep original confidence but lower it slightly
                        confidence = max(0.1, confidence * 0.7)
                        vietnamese_confidence = max(0.1, vietnamese_confidence * 0.7)
                    else:
                        # Only use "KhÃ´ng cÃ³ lá»i thoáº¡i" if truly no content
                        logger.warning("âš ï¸ No speech content detected")
                        corrected_text = "KhÃ´ng cÃ³ lá»i thoáº¡i"
                        confidence = 0.0
                        vietnamese_confidence = 0.0
                
                return {
                    'success': True,
                    'transcript': corrected_text,
                    'confidence': confidence,
                    'vietnamese_confidence': vietnamese_confidence,
                    'segments': 1,
                    'duration': duration,
                    'model': transcription_model + (' + gpt-4o' if used_gpt else ''),
                    'language': language,
                    'use_vietnamese_asr': False,
                    'processing_time': total_processing_time,
                    'whisper_time': whisper_processing_time,
                    'gpt4o_time': gpt4o_processing_time,
                    'original_text': text,
                    'improved_text': improved_text,
                    'is_suspicious': is_suspicious
                }
                
            except Exception as openai_error:
                logger.error(f"âŒ OpenAI transcription failed: {openai_error}")
                return self._error_result(f"OpenAI transcription failed: {openai_error}")
            
        except Exception as e:
            logger.error(f"âŒ Transcription failed: {e}")
            return self._error_result(str(e))
    

    
    def _improve_with_gpt4o(self, text: str, language: str = 'vi') -> str:
        """Cáº£i thiá»‡n transcript báº±ng GPT-4o"""
        try:
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key or not text.strip():
                return text
            
            from openai import OpenAI
            client = OpenAI(api_key=openai_api_key)
            
            if language == 'vi':
                prompt = f"""
                Cáº£i thiá»‡n transcript tiáº¿ng Viá»‡t sau Ä‘Ã¢y má»™t cÃ¡ch THáº¬N TRá»ŒNG:
                - Chá»‰ sá»­a lá»—i chÃ­nh táº£ rÃµ rÃ ng (vÃ­ dá»¥: "toi" -> "tÃ´i")
                - ThÃªm dáº¥u cÃ¢u cÆ¡ báº£n náº¿u thiáº¿u
                - KHÃ”NG thÃªm tá»« má»›i hoáº·c thay Ä‘á»•i Ã½ nghÄ©a
                - KHÃ”NG thÃªm ná»™i dung khÃ´ng cÃ³ trong audio gá»‘c
                - Giá»¯ nguyÃªn Ä‘á»™ dÃ i vÃ  cáº¥u trÃºc cÃ¢u
                
                Transcript gá»‘c: "{text}"
                
                Chá»‰ tráº£ vá» text Ä‘Ã£ cáº£i thiá»‡n, khÃ´ng thÃªm giáº£i thÃ­ch. Náº¿u transcript quÃ¡ ngáº¯n hoáº·c khÃ´ng rÃµ rÃ ng, tráº£ vá» nguyÃªn báº£n.
                """
                system_content = "Báº¡n lÃ  chuyÃªn gia cáº£i thiá»‡n transcript tiáº¿ng Viá»‡t. Chá»‰ sá»­a lá»—i chÃ­nh táº£ vÃ  dáº¥u cÃ¢u, KHÃ”NG thÃªm ná»™i dung má»›i."
            else:
                prompt = f"""
                Carefully improve the following English transcript:
                - Only fix obvious spelling errors
                - Add basic punctuation if missing
                - DO NOT add new words or change meaning
                - DO NOT add content not in the original audio
                - Keep the same length and sentence structure
                
                Original transcript: "{text}"
                
                Only return the improved text, no explanations. If the transcript is too short or unclear, return the original.
                """
                system_content = "You are an expert in improving English transcripts. Only fix spelling and punctuation, DO NOT add new content."
            
            response = client.chat.completions.create(
                model="gpt-4o",  # Sá»­ dá»¥ng GPT-4o Ä‘á»ƒ cÃ³ cháº¥t lÆ°á»£ng tá»‘t nháº¥t
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.1
            )

            # Safely extract response content
            message_content = response.choices[0].message.content
            if message_content is None:
                logger.warning("âš ï¸ GPT-4o returned None content")
                return text

            improved_text = message_content.strip()
            if improved_text and improved_text != text:
                logger.info("âœ¨ GPT-4o improved transcript quality")
                return improved_text

            return text
            
        except Exception as e:
            logger.warning(f"âš ï¸ GPT-4o improvement failed: {e}")
            return text
    
    def _chunk_audio(self, waveform: torch.Tensor, sample_rate: int) -> List[torch.Tensor]:
        """Chia audio thÃ nh chunks Ä‘á»ƒ xá»­ lÃ½ tá»‘t hÆ¡n"""
        duration = len(waveform) / sample_rate
        
        # Náº¿u audio ngáº¯n hÆ¡n 10 giÃ¢y, xá»­ lÃ½ toÃ n bá»™ má»™t láº§n
        if duration <= 10.0:
            logger.info(f"ğŸµ Audio duration {duration:.1f}s <= 10s, processing as single chunk")
            return [waveform]
        
        # Náº¿u audio dÃ i hÆ¡n, chia thÃ nh chunks
        chunk_samples = int(self.config.chunk_duration * sample_rate)
        overlap_samples = int(self.config.overlap_duration * sample_rate)
        
        chunks = []
        start = 0
        max_chunks = 3  # Giáº£m xuá»‘ng 3 chunks Ä‘á»ƒ trÃ¡nh treo
        
        while start < len(waveform) and len(chunks) < max_chunks:
            end = min(start + chunk_samples, len(waveform))
            chunk = waveform[start:end]
            
            # Ensure minimum chunk size - tÄƒng lÃªn 1 giÃ¢y
            if len(chunk) > sample_rate * 1.0:  # At least 1 second
                chunks.append(chunk)
                logger.info(f"ğŸ“¦ Created chunk {len(chunks)}: {len(chunk)} samples ({len(chunk)/sample_rate:.1f}s)")
            
            start = end - overlap_samples
            if start >= len(waveform):
                break
        
        logger.info(f"ğŸ¯ Total chunks created: {len(chunks)}")
        return chunks
    
    def _transcribe_segment(self, audio_segment: torch.Tensor, sample_rate: int) -> Dict[str, Any]:
        """Transcribe má»™t segment audio - ÄÆ N GIáº¢N HÃ“A"""
        try:
            import torch
            import time
            import threading
            
            logger.info(f"ğŸµ Processing segment: {len(audio_segment)} samples")
            
            # Prepare input
            inputs = self.processor(
                audio_segment.numpy(),
                sampling_rate=sample_rate,
                return_tensors="pt"
            )
            
            logger.info("âœ… Input prepared, starting generation...")
            
            # Generate transcription vá»›i timeout dÃ i hÆ¡n vÃ  error handling tá»‘t hÆ¡n
            start_time = time.time()
            
            result = None
            error = None
            
            def generate_with_timeout():
                nonlocal result, error
                try:
                    with torch.no_grad():
                        result = self.model.generate(
                            inputs["input_features"],
                            language="vi",
                            task="transcribe",
                            max_length=128,  # Ráº¥t ngáº¯n
                            num_beams=1,     # Nhanh nháº¥t
                            temperature=0.0,
                            do_sample=False,
                            max_time=25.0    # TÄƒng timeout lÃªn 25 giÃ¢y
                        )
                except Exception as e:
                    error = e
            
            # Start generation in a separate thread
            thread = threading.Thread(target=generate_with_timeout)
            thread.daemon = True
            thread.start()
            
            # Wait for completion or timeout
            thread.join(timeout=60)  # TÄƒng timeout lÃªn 60 giÃ¢y
            
            if thread.is_alive():
                logger.error("âŒ Transcription timeout after 60 seconds")
                raise TimeoutError("Transcription timeout after 60 seconds")
            
            if error:
                raise error
            
            generated_ids = result
            
            end_time = time.time()
            logger.info(f"âœ… Generation completed in {end_time - start_time:.2f}s")
            
            # Decode
            transcription = self.processor.batch_decode(
                generated_ids, 
                skip_special_tokens=True
            )[0]
            
            # Calculate confidence
            confidence = self._estimate_confidence(transcription, audio_segment)
            
            return {
                'success': True,
                'text': transcription.strip(),
                'confidence': confidence,
                'duration': len(audio_segment) / sample_rate
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Segment transcription failed: {e}")
            return {
                'success': False,
                'text': '',
                'confidence': 0.0,
                'error': str(e)
            }
    
    def _combine_segments(self, segments: List[Dict]) -> str:
        """Káº¿t há»£p cÃ¡c segments thÃ nh transcript hoÃ n chá»‰nh"""
        texts = []
        
        for segment in segments:
            text = segment.get('text', '').strip()
            if text and segment.get('confidence', 0) >= self.config.min_confidence:
                texts.append(text)
        
        # Join and clean up
        combined = ' '.join(texts)
        
        # Remove duplicate words at boundaries
        words = combined.split()
        cleaned_words = []
        
        for i, word in enumerate(words):
            if i == 0 or word != words[i-1]:
                cleaned_words.append(word)
        
        return ' '.join(cleaned_words)
    
    def _estimate_confidence(self, text: str, audio: torch.Tensor) -> float:
        """Æ¯á»›c tÃ­nh confidence score"""
        try:
            # Base confidence tá»« text quality
            text_confidence = self.language_model.calculate_vietnamese_confidence(text)
            
            # Audio quality score (simplified)
            audio_quality = min(1.0, torch.std(audio).item() * 2)
            
            # Combine scores
            final_confidence = (text_confidence * 0.8 + audio_quality * 0.2)
            
            return round(final_confidence, 3)
            
        except Exception:
            return 0.5
    
    def start_real_time_transcription(self, callback: Callable = None):
        """Báº¯t Ä‘áº§u transcription real-time"""
        if callback:
            self.config.real_time_callback = callback
        
        self.is_processing = True
        processing_thread = threading.Thread(target=self._process_audio_queue)
        processing_thread.daemon = True
        processing_thread.start()
        
        logger.info("ğŸ™ï¸ Real-time transcription started")
    
    def stop_real_time_transcription(self):
        """Dá»«ng transcription real-time"""
        self.is_processing = False
        logger.info("â¹ï¸ Real-time transcription stopped")
    
    def add_audio_chunk(self, audio_data: bytes, timestamp: float = None):
        """ThÃªm audio chunk vÃ o queue Ä‘á»ƒ xá»­ lÃ½"""
        if timestamp is None:
            timestamp = time.time()
        
        self.audio_queue.put((audio_data, timestamp))
    
    def _process_audio_queue(self):
        """Xá»­ lÃ½ audio queue trong background"""
        while self.is_processing:
            try:
                if not self.audio_queue.empty():
                    audio_data, timestamp = self.audio_queue.get(timeout=1.0)
                    
                    # Process audio chunk
                    future = self.executor.submit(self._process_audio_chunk, audio_data, timestamp)
                    
                else:
                    time.sleep(0.1)
                    
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"âŒ Audio queue processing error: {e}")
    
    def _process_audio_chunk(self, audio_data: bytes, timestamp: float):
        """Xá»­ lÃ½ má»™t audio chunk"""
        try:
            # Convert bytes to audio file (temporary)
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
                tmp.write(audio_data)
                tmp_path = tmp.name
            
            # Transcribe
            result = self.transcribe_audio_file(tmp_path)
            
            # Clean up
            os.unlink(tmp_path)
            
            # Call callback if available
            if self.config.real_time_callback and result['success']:
                self.config.real_time_callback(result, timestamp)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Audio chunk processing failed: {e}")
            return self._error_result(str(e))
    
    def _apply_comprehensive_vietnamese_corrections(self, text: str, question: str = None) -> str:
        """Apply comprehensive Vietnamese corrections in optimal order with context awareness"""
        if not text:
            return text

        try:
            # DISABLED: Comprehensive corrections logging

            # Step 1: Normalize text
            corrected_text = text.lower().strip()

            # Step 2: ALL CORRECTIONS DISABLED - Return text as-is
            # DISABLED: All corrections logging
            corrected_text = text.lower().strip()

            # DISABLED: Skip all correction steps to preserve original transcript
            # corrected_text = self._apply_single_letter_spelling_corrections(corrected_text)
            # corrected_text = self._apply_problematic_words_corrections(corrected_text)
            # corrected_text = self._apply_pronunciation_corrections(corrected_text)
            # corrected_text = self._apply_regional_corrections(corrected_text)
            # corrected_text = self._apply_common_asr_corrections(corrected_text)
            # corrected_text = self._apply_name_database_corrections(corrected_text)
            # corrected_text = self._apply_spelling_to_names(corrected_text)
            # corrected_text = self._apply_vietnamese_spelling_corrections(corrected_text)
            # corrected_text = self._apply_context_improvements(corrected_text)
            # corrected_text = self._apply_specialized_corrections(corrected_text)
            # corrected_text = self._apply_final_formatting(corrected_text, text)
            # DISABLED: Final formatting logging

            # DISABLED: Comprehensive corrections completion logging
            return corrected_text

        except Exception as e:
            # DISABLED: Comprehensive corrections error logging
            return text

    def _apply_name_database_corrections(self, text: str) -> str:
        """Apply name corrections trained from Vietnamese Name Database"""
        try:
            # Load name database if not already loaded
            if not hasattr(self, '_name_corrections_cache'):
                self._name_corrections_cache = self._load_vietnamese_name_database()
                # DISABLED: Name corrections loaded logging

            name_corrections = self._name_corrections_cache

            # Debug: Show first few corrections
            if len(name_corrections) > 0:
                sample_corrections = list(name_corrections.items())[:3]
                # DISABLED: Sample corrections logging

            original_text = text

            # Apply name corrections to full names first (more specific)
            for unsigned, signed in name_corrections.items():
                if len(unsigned.split()) >= 2:  # Only full names (surname + given name)
                    text = text.replace(unsigned, signed)
                    if text != original_text:
                        # DISABLED: Full name corrected logging
                        pass

            # Then apply single name corrections
            words = text.split()
            corrected_words = []

            for word in words:
                # Check if single word needs name correction
                if word in name_corrections:
                    corrected_words.append(name_corrections[word])
                    # DISABLED: Single name corrected logging
                else:
                    corrected_words.append(word)

            result = ' '.join(corrected_words)

            if result != original_text:
                # DISABLED: Name corrections applied logging
                pass
            else:
                # DISABLED: No name corrections logging
                pass

            return result

        except Exception as e:
            # DISABLED: Name database corrections error logging
            return text

    def _apply_single_letter_spelling_corrections(self, text: str) -> str:
        """Apply corrections for single letter spelling pronunciations - DISABLED"""
        # DISABLED: Return text as-is without spelling corrections
        return text

        try:
            # Simple word replacements for common misrecognitions
            spelling_corrections = {
                # === NGUYÃŠN Ã‚M (Vowels) ===

                # A variations
                "a": "A", "Ã ": "A", "Ã¡": "A", "áº£": "A", "Ã£": "A", "áº¡": "A",
                "Äƒ": "A", "áº±": "A", "áº¯": "A", "áº³": "A", "áºµ": "A", "áº·": "A",
                "Ã¢": "A", "áº§": "A", "áº¥": "A", "áº©": "A", "áº«": "A", "áº­": "A",

                # E variations
                "e": "E", "Ã¨": "E", "Ã©": "E", "áº»": "E", "áº½": "E", "áº¹": "E",
                "Ãª": "E", "á»": "E", "áº¿": "E", "á»ƒ": "E", "á»…": "E", "á»‡": "E",

                # I variations
                "i": "I", "Ã¬": "I", "Ã­": "I", "á»‰": "I", "Ä©": "I", "á»‹": "I",

                # O variations
                "o": "O", "Ã²": "O", "Ã³": "O", "á»": "O", "Ãµ": "O", "á»": "O",
                "Ã´": "O", "á»“": "O", "á»‘": "O", "á»•": "O", "á»—": "O", "á»™": "O",
                "Æ¡": "O", "á»": "O", "á»›": "O", "á»Ÿ": "O", "á»¡": "O", "á»£": "O",

                # U variations
                "u": "U", "Ã¹": "U", "Ãº": "U", "á»§": "U", "Å©": "U", "á»¥": "U",
                "Æ°": "U", "á»«": "U", "á»©": "U", "á»­": "U", "á»¯": "U", "á»±": "U",

                # Y variations
                "y": "Y", "á»³": "Y", "Ã½": "Y", "á»·": "Y", "á»¹": "Y", "á»µ": "Y",

                # === PHá»¤ Ã‚M (Consonants) ===

                # B variations
                "bÃª": "B", "báº¿": "B", "bÃ©": "B", "bÃ¬": "B", "bÃ­": "B", "bÃ¨": "B", "bÃ©": "B",
                "b": "B", "bá»": "B", "bÆ¡": "B", "bÃ³": "B", "bá»›": "B", "be": "B", "bÃª": "B",

                # C variations
                "cá»": "C", "co": "C", "cÆ¡": "C", "cÃ³": "C", "cá»›": "C", "cÃ ": "C", "cÃ¡": "C",
                "c": "C", "cÃª": "C", "cÃ©": "C", "cÃ¬": "C", "cÃ­": "C", "ce": "C", "ci": "C",

                # D variations (regular D)
                "dÃª": "D", "dÃ©": "D", "dÃ¨": "D", "dÃ¬": "D", "dÃ­": "D", "dá»": "D", "dÆ¡": "D",
                "d": "D", "do": "D", "dÃ³": "D", "dá»›": "D", "de": "D", "di": "D", "dÃª": "D",

                # Ä variations (special D with bar)
                "Ä‘Ãª": "Ä", "Ä‘áº¿": "Ä", "Ä‘Ã©": "Ä", "Ä‘Ã¨": "Ä", "Ä‘Ã¬": "Ä", "Ä‘Ã­": "Ä", "Ä‘á»": "Ä", "Ä‘Æ¡": "Ä",
                "Ä‘": "Ä", "Ä‘o": "Ä", "Ä‘Ã³": "Ä", "Ä‘á»›": "Ä", "Ä‘e": "Ä", "Ä‘i": "Ä", "Ä‘Ãª": "Ä",

                # G variations
                "gá»": "G", "gÆ¡": "G", "gÃ³": "G", "gá»›": "G", "gÃ ": "G", "gÃ¡": "G", "gÃ¬": "G",
                "g": "G", "go": "G", "gÃª": "G", "gÃ©": "G", "ge": "G", "gi": "G", "gÃª": "G",

                # H variations
                "hÃª": "H", "háº¿": "H", "hÃ©": "H", "hÃ¬": "H", "hÃ­": "H", "hÃ¨": "H", "hÃ©": "H",
                "hÃ´": "H", "há»‘": "H", "há»›": "H", "hÃ ": "H", "hÃ¡": "H", "há»": "H", "hÆ¡": "H",
                "h": "H", "ho": "H", "hu": "H", "hÃº": "H", "há»§": "H", "há»©": "H", "he": "H", "hi": "H",

                # K variations
                "ká»": "K", "kÆ¡": "K", "kÃ³": "K", "ká»›": "K", "kÃ ": "K", "kÃ¡": "K", "kÃ¬": "K",
                "k": "K", "ko": "K", "kÃª": "K", "kÃ©": "K", "ke": "K", "ki": "K", "kÃª": "K",

                # L variations (removed "lÃ " and "lÃ¡" to avoid confusion with common Vietnamese words)
                "lá»": "L", "lÆ¡": "L", "lÃ³": "L", "lá»›": "L", "lÃ¬": "L",
                "l": "L", "lo": "L", "lÃª": "L", "lÃ©": "L", "le": "L", "li": "L", "lÃª": "L",

                # M variations
                "má»": "M", "mÆ¡": "M", "mÃ³": "M", "má»›": "M", "mÃ ": "M", "mÃ¡": "M", "mÃ¬": "M",
                "m": "M", "mo": "M", "mÃª": "M", "mÃ©": "M", "me": "M", "mi": "M", "mÃª": "M",

                # N variations
                "ná»": "N", "nÆ¡": "N", "nÃ³": "N", "ná»›": "N", "nÃ ": "N", "nÃ¡": "N", "nÃ¬": "N",
                "n": "N", "no": "N", "nÃª": "N", "nÃ©": "N", "ne": "N", "ni": "N", "nÃª": "N",

                # P variations
                "pÃª": "P", "páº¿": "P", "pÃ©": "P", "pÃ¬": "P", "pÃ­": "P", "pÃ¨": "P", "pÃ©": "P",
                "p": "P", "pá»": "P", "pÆ¡": "P", "pÃ³": "P", "pá»›": "P", "pe": "P", "pi": "P",

                # Q variations
                "qá»": "Q", "qÆ¡": "Q", "qÃ³": "Q", "qá»›": "Q", "qÃ ": "Q", "qÃ¡": "Q", "qÃ¬": "Q",
                "q": "Q", "qo": "Q", "qÃª": "Q", "qÃ©": "Q", "qe": "Q", "qi": "Q", "qÃª": "Q",

                # R variations
                "rá»": "R", "rÆ¡": "R", "rÃ³": "R", "rá»›": "R", "rÃ ": "R", "rÃ¡": "R", "rÃ¬": "R",
                "r": "R", "ro": "R", "rÃª": "R", "rÃ©": "R", "re": "R", "ri": "R", "rÃª": "R",

                # S variations
                "sá»": "S", "sÆ¡": "S", "sÃ³": "S", "sá»›": "S", "sÃ ": "S", "sÃ¡": "S", "sÃ¬": "S",
                "s": "S", "so": "S", "sÃª": "S", "sÃ©": "S", "se": "S", "si": "S", "sÃª": "S",

                # T variations
                "tá»": "T", "tÆ¡": "T", "tÃ³": "T", "tá»›": "T", "tÃ ": "T", "tÃ¡": "T", "tÃ¬": "T",
                "t": "T", "to": "T", "tÃª": "T", "tÃ©": "T", "te": "T", "ti": "T", "tÃª": "T",

                # V variations
                "vá»": "V", "vÆ¡": "V", "vÃ³": "V", "vá»›": "V", "vÃ ": "V", "vÃ¡": "V", "vÃ¬": "V",
                "v": "V", "vo": "V", "vÃª": "V", "vÃ©": "V", "ve": "V", "vi": "V", "vÃª": "V",

                # X variations
                "xá»": "X", "xÆ¡": "X", "xÃ³": "X", "xá»›": "X", "xÃ ": "X", "xÃ¡": "X", "xÃ¬": "X",
                "x": "X", "xo": "X", "xÃª": "X", "xÃ©": "X", "xe": "X", "xi": "X", "xÃª": "X",
            }

            corrected_text = text.lower()

            # Apply single word replacements
            for wrong, correct in spelling_corrections.items():
                # Handle words with spaces around
                corrected_text = corrected_text.replace(f" {wrong} ", f" {correct} ")
                corrected_text = corrected_text.replace(f" {wrong}.", f" {correct}.")
                corrected_text = corrected_text.replace(f" {wrong},", f" {correct},")
                corrected_text = corrected_text.replace(f" {wrong}!", f" {correct}!")
                corrected_text = corrected_text.replace(f" {wrong}?", f" {correct}?")

                # Handle words at the beginning of text
                if corrected_text.startswith(f"{wrong} "):
                    corrected_text = corrected_text.replace(f"{wrong} ", f"{correct} ", 1)
                if corrected_text == wrong:
                    corrected_text = correct

            # Handle phrase replacements - Common spelling patterns
            phrase_corrections = {
                # Common spelling patterns for names
                "bÃª hÃ©t hÃ´ cÆ¡": "B H O C",
                "bÃ© hÃ©t hu cá»": "B H U C",
                "bÃª hÃ©t hu cá»": "B H U C",
                "pÃª hÃ¡t u cá»": "P H U C",
                "pÃª hÃ¡t u cÆ¡": "P H U C",
                "pÃª hÃ©t u cá»": "P H U C",
                "pÃª hÃ©t u cÆ¡": "P H U C",

                # Additional common patterns
                "Ãª hÃ¡t": "H A",
                "Ãª hÃ¡t u": "H U",
                "Ãª hÃ¡t u cá»": "H U C",
                "Ãª hÃ©t": "H E",
                "Ãª hÃ©t u": "H U",
                "Ãª hÃ©t u cá»": "H U C",

                # More comprehensive patterns for all letters
                "dÃª": "D",
                "gá»": "G",
                "ká»": "K",
                "lá»": "L",
                "má»": "M",
                "ná»": "N",
                "qá»": "Q",
                "rá»": "R",
                "sá»": "S",
                "tá»": "T",
                "vá»": "V",
                "xá»": "X",

                # Combined patterns for spelling sequences
                "bÃª Ãª": "B E",
                "cá» Ãª": "C E",
                "dÃª Ãª": "D E",
                "gá» Ãª": "G E",
                "pÃª Ãª": "P E",
                "tá» Ãª": "T E",
                "vá» Ãª": "V E",

                "bÃª a": "B A",
                "cá» a": "C A",
                "dÃª a": "D A",
                "gá» a": "G A",
                "pÃª a": "P A",
                "tá» a": "T A",
                "vá» a": "V A",

                "bÃª i": "B I",
                "cá» i": "C I",
                "dÃª i": "D I",
                "gá» i": "G I",
                "pÃª i": "P I",
                "tá» i": "T I",
                "vá» i": "V I",

                "bÃª o": "B O",
                "cá» o": "C O",
                "dÃª o": "D O",
                "gá» o": "G O",
                "pÃª o": "P O",
                "tá» o": "T O",
                "vá» o": "V O",

                "bÃª u": "B U",
                "cá» u": "C U",
                "dÃª u": "D U",
                "gá» u": "G U",
                "pÃª u": "P U",
                "tá» u": "T U",
                "vá» u": "V U",

                "bÃª y": "B Y",
                "cá» y": "C Y",
                "dÃª y": "D Y",
                "gá» y": "G Y",
                "pÃª y": "P Y",
                "tá» y": "T Y",
                "vá» y": "V Y",

                # Complex spelling sequences
                "bÃª cá» dá»": "B C D",
                "pÃª há» u cá»": "P H U C",
                "tá» Ãª o": "T E O",
                "vá» Äƒ n": "V A N",
                "gá» i Äƒ n g": "G I A N G",
            }

            for wrong_phrase, correct_phrase in phrase_corrections.items():
                corrected_text = corrected_text.replace(wrong_phrase, correct_phrase)

            if corrected_text != text.lower():
                # DISABLED: Single letter spelling corrections logging
                pass

            return corrected_text

        except Exception as e:
            # DISABLED: Single letter spelling corrections error logging
            return text

    def _apply_problematic_words_corrections(self, text: str) -> str:
        """Apply corrections for words that commonly cause conflicts first"""
        try:
            problematic_corrections = {
                # Words that conflict with other corrections - handle these first
                " toi ": " tÃ´i ",  # Ensure space to avoid conflicts
                " toi.": " tÃ´i.",  # Handle end of sentence
                " toi!": " tÃ´i!",  # Handle exclamation
                " toi?": " tÃ´i?",  # Handle question
                " toi,": " tÃ´i,",  # Handle comma

                " ban ": " báº¡n ",  # Ensure space to avoid conflicts
                " ban.": " báº¡n.",  # Handle end of sentence
                " ban!": " báº¡n!",  # Handle exclamation
                " ban?": " báº¡n?",  # Handle question
                " ban,": " báº¡n,",  # Handle comma

                " viet nam ": " Viá»‡t Nam ",  # Proper noun
                " troi ": " trá»i ",  # Weather/sky
                " thich ": " thÃ­ch ",  # Like/enjoy
                " muon ": " muá»‘n ",  # Want
                " cam thay ": " cáº£m tháº¥y ",  # Feel
                " rat vui ": " ráº¥t vui ",  # Very happy
                " bac si ": " bÃ¡c sÄ© ",  # Doctor
                " benh vien ": " bá»‡nh viá»‡n ",  # Hospital
                " dau dau ": " Ä‘au Ä‘áº§u ",  # Headache
                " uong thuoc ": " uá»‘ng thuá»‘c ",  # Take medicine

                # Numbers that might conflict
                " mot ": " má»™t ",
                " hai ": " hai ",
                " nam ": " nÄƒm ",
                " tuoi ": " tuá»•i ",

                # Additional common problematic words
                " co gai ": " cÃ´ gÃ¡i ",  # Avoid "cÆ¡ gai"
                " co giao ": " cÃ´ giÃ¡o ",  # Avoid "cÆ¡ giao"
                " nguoi ban ": " ngÆ°á»i báº¡n ",  # Avoid conflicts
                " ban than ": " báº¡n thÃ¢n ",  # Avoid conflicts
            }

            for incorrect, correct in problematic_corrections.items():
                text = text.replace(incorrect, correct)

            return text

        except Exception as e:
            # DISABLED: Problematic words corrections error logging
            return text

    def _load_vietnamese_name_database(self):
        """Load Vietnamese name database for training name corrections"""
        try:
            name_corrections = {}
            name_db_path = os.path.join(os.path.dirname(__file__), '..', 'vietnamese-namedb')

            # Load boy names
            boy_file = os.path.join(name_db_path, 'boy.txt')
            if os.path.exists(boy_file):
                with open(boy_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        name = line.strip()
                        if name:
                            # Create multiple mappings for better matching
                            unsigned_name = self._convert_to_unsigned(name)
                            lowercase_name = name.lower()
                            lowercase_unsigned = unsigned_name.lower()

                            # Store all variations
                            if unsigned_name != name:
                                name_corrections[unsigned_name] = name
                            if lowercase_name != name:
                                name_corrections[lowercase_name] = name
                            if lowercase_unsigned != name:
                                name_corrections[lowercase_unsigned] = name

                            # Also store the original name for exact matches
                            name_corrections[name] = name

            # Load girl names
            girl_file = os.path.join(name_db_path, 'girl.txt')
            if os.path.exists(girl_file):
                with open(girl_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        name = line.strip()
                        if name:
                            unsigned_name = self._convert_to_unsigned(name)
                            lowercase_name = name.lower()
                            lowercase_unsigned = unsigned_name.lower()

                            # Store all variations
                            if unsigned_name != name:
                                name_corrections[unsigned_name] = name
                            if lowercase_name != name:
                                name_corrections[lowercase_name] = name
                            if lowercase_unsigned != name:
                                name_corrections[lowercase_unsigned] = name

                            # Also store the original name for exact matches
                            name_corrections[name] = name

            # Load JSON database for full names
            json_file = os.path.join(name_db_path, 'uit_member.json')
            if os.path.exists(json_file):
                import json
                with open(json_file, 'r', encoding='utf-8') as f:
                    members = json.load(f)
                    for member in members:
                        if 'full_name' in member:
                            full_name = member['full_name']
                            unsigned_full_name = self._convert_to_unsigned(full_name)
                            lowercase_full_name = full_name.lower()
                            lowercase_unsigned_full = unsigned_full_name.lower()

                            # Store all variations for full names
                            if unsigned_full_name != full_name:
                                name_corrections[unsigned_full_name] = full_name
                            if lowercase_full_name != full_name:
                                name_corrections[lowercase_full_name] = full_name
                            if lowercase_unsigned_full != full_name:
                                name_corrections[lowercase_unsigned_full] = full_name

                        # Handle first and last names
                        if 'first_name' in member and 'last_name' in member:
                            first_name = member['first_name']
                            last_name = member['last_name']

                            # Create full name from parts
                            if last_name and first_name:
                                combined_name = f"{last_name} {first_name}"
                                unsigned_combined = self._convert_to_unsigned(combined_name)
                                lowercase_combined = combined_name.lower()

                                if unsigned_combined != combined_name:
                                    name_corrections[unsigned_combined] = combined_name
                                if lowercase_combined != combined_name:
                                    name_corrections[lowercase_combined] = combined_name

            # Add some common name corrections manually for better coverage
            manual_corrections = {
                "nguyen van minh": "Nguyá»…n VÄƒn Minh",
                "tran thi mai": "Tráº§n Thá»‹ Mai",
                "le hoang quan": "LÃª HoÃ ng QuÃ¢n",
                "pham van duc": "Pháº¡m VÄƒn Äá»©c",
                "hoang thi lan": "HoÃ ng Thá»‹ Lan",
                "nguyen van nam": "Nguyá»…n VÄƒn Nam",
                "tran thi thu": "Tráº§n Thá»‹ Thu",
                "le thi hoa": "LÃª Thá»‹ Hoa",
                "pham thi linh": "Pháº¡m Thá»‹ Linh",
                "hoang van tung": "HoÃ ng VÄƒn TÃ¹ng",
                "nguyen van anh": "Nguyá»…n VÄƒn Anh",
                "tran thi linh": "Tráº§n Thá»‹ Linh",
            }

            for unsigned, signed in manual_corrections.items():
                name_corrections[unsigned] = signed

            # DISABLED: Vietnamese Name DB loaded logging
            return name_corrections

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load Vietnamese name database: {e}")
            return {}

    def _convert_to_unsigned(self, text: str) -> str:
        """Convert Vietnamese text with accents to unsigned version"""
        try:
            # Vietnamese character mapping
            vietnamese_map = {
                'Ã¡': 'a', 'Ã ': 'a', 'áº£': 'a', 'Ã£': 'a', 'áº¡': 'a',
                'Äƒ': 'a', 'áº¯': 'a', 'áº±': 'a', 'áº³': 'a', 'áºµ': 'a', 'áº·': 'a',
                'Ã¢': 'a', 'áº¥': 'a', 'áº§': 'a', 'áº©': 'a', 'áº«': 'a', 'áº­': 'a',
                'Ã©': 'e', 'Ã¨': 'e', 'áº»': 'e', 'áº½': 'e', 'áº¹': 'e',
                'Ãª': 'e', 'áº¿': 'e', 'á»': 'e', 'á»ƒ': 'e', 'á»…': 'e', 'á»‡': 'e',
                'Ã­': 'i', 'Ã¬': 'i', 'á»‰': 'i', 'Ä©': 'i', 'á»‹': 'i',
                'Ã³': 'o', 'Ã²': 'o', 'á»': 'o', 'Ãµ': 'o', 'á»': 'o',
                'Ã´': 'o', 'á»‘': 'o', 'á»“': 'o', 'á»•': 'o', 'á»—': 'o', 'á»™': 'o',
                'Æ¡': 'o', 'á»›': 'o', 'á»': 'o', 'á»Ÿ': 'o', 'á»¡': 'o', 'á»£': 'o',
                'Ãº': 'u', 'Ã¹': 'u', 'á»§': 'u', 'Å©': 'u', 'á»¥': 'u',
                'Æ°': 'u', 'á»©': 'u', 'á»«': 'u', 'á»­': 'u', 'á»¯': 'u', 'á»±': 'u',
                'Ã½': 'y', 'á»³': 'y', 'á»·': 'y', 'á»¹': 'y', 'á»µ': 'y',
                'Ä‘': 'd',
                'Ã': 'A', 'Ã€': 'A', 'áº¢': 'A', 'Ãƒ': 'A', 'áº ': 'A',
                'Ä‚': 'A', 'áº®': 'A', 'áº°': 'A', 'áº²': 'A', 'áº´': 'A', 'áº¶': 'A',
                'Ã‚': 'A', 'áº¤': 'A', 'áº¦': 'A', 'áº¨': 'A', 'áºª': 'A', 'áº¬': 'A',
                'Ã‰': 'E', 'Ãˆ': 'E', 'áºº': 'E', 'áº¼': 'E', 'áº¸': 'E',
                'ÃŠ': 'E', 'áº¾': 'E', 'á»€': 'E', 'á»‚': 'E', 'á»„': 'E', 'á»†': 'E',
                'Ã': 'I', 'ÃŒ': 'I', 'á»ˆ': 'I', 'Ä¨': 'I', 'á»Š': 'I',
                'Ã“': 'O', 'Ã’': 'O', 'á»': 'O', 'Ã•': 'O', 'á»Œ': 'O',
                'Ã”': 'O', 'á»': 'O', 'á»’': 'O', 'á»”': 'O', 'á»–': 'O', 'á»˜': 'O',
                'Æ ': 'O', 'á»š': 'O', 'á»œ': 'O', 'á»': 'O', 'á» ': 'O', 'á»¢': 'O',
                'Ãš': 'U', 'Ã™': 'U', 'á»¦': 'U', 'Å¨': 'U', 'á»¤': 'U',
                'Æ¯': 'U', 'á»¨': 'U', 'á»ª': 'U', 'á»¬': 'U', 'á»®': 'U', 'á»°': 'U',
                'Ã': 'Y', 'á»²': 'Y', 'á»¶': 'Y', 'á»¸': 'Y', 'á»´': 'Y',
                'Ä': 'D'
            }

            result = text
            for accented, plain in vietnamese_map.items():
                result = result.replace(accented, plain)

            return result

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to convert to unsigned: {e}")
            return text

    def _find_signed_equivalent(self, unsigned_name: str) -> str:
        """Find signed equivalent from name database"""
        try:
            # This is a simplified version - in practice you'd want to search the database
            # For now, return None and let the main corrections handle it
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to find signed equivalent: {e}")
            return None

    def _create_spelling_pronunciation(self, name: str) -> str:
        """Create spelling pronunciation for Vietnamese names (e.g., 'PhÃºc' â†’ 'P H U C PhÃºc')"""
        if not name or len(name.strip()) == 0:
            return name

        try:
            # Convert to uppercase for spelling, but keep original for reference
            name_upper = name.upper()
            name_original = name

            # Create spelling by breaking down each character
            spelling_parts = []

            for char in name_upper:
                if char.isalpha():
                    # For Vietnamese characters with diacritics, we need special handling
                    if char in ['Ã', 'Ã€', 'áº¢', 'Ãƒ', 'áº ']:
                        spelling_parts.append('A')
                    elif char in ['Ä‚', 'áº®', 'áº°', 'áº²', 'áº´', 'áº¶']:
                        spelling_parts.append('A')
                    elif char in ['Ã‚', 'áº¤', 'áº¦', 'áº¨', 'áºª', 'áº¬']:
                        spelling_parts.append('A')
                    elif char in ['Ã‰', 'Ãˆ', 'áºº', 'áº¼', 'áº¸']:
                        spelling_parts.append('E')
                    elif char in ['ÃŠ', 'áº¾', 'á»€', 'á»‚', 'á»„', 'á»†']:
                        spelling_parts.append('E')
                    elif char in ['Ã', 'ÃŒ', 'á»ˆ', 'Ä¨', 'á»Š']:
                        spelling_parts.append('I')
                    elif char in ['Ã“', 'Ã’', 'á»', 'Ã•', 'á»Œ']:
                        spelling_parts.append('O')
                    elif char in ['Ã”', 'á»', 'á»’', 'á»”', 'á»–', 'á»˜']:
                        spelling_parts.append('O')
                    elif char in ['Æ ', 'á»š', 'á»œ', 'á»', 'á» ', 'á»¢']:
                        spelling_parts.append('O')
                    elif char in ['Ãš', 'Ã™', 'á»¦', 'Å¨', 'á»¤']:
                        spelling_parts.append('U')
                    elif char in ['Æ¯', 'á»¨', 'á»ª', 'á»¬', 'á»®', 'á»°']:
                        spelling_parts.append('U')
                    elif char in ['Ã', 'á»²', 'á»¶', 'á»¸', 'á»´']:
                        spelling_parts.append('Y')
                    elif char == 'Ä':
                        spelling_parts.append('D')
                    else:
                        # Regular ASCII letters
                        spelling_parts.append(char)
                elif char.isspace():
                    spelling_parts.append(' ')
                # Skip other characters like punctuation

            # Join spelling parts
            spelling = ' '.join(spelling_parts).strip()

            # Return format: "P H U C PhÃºc"
            if spelling and spelling != name_upper:
                return f"{spelling} {name_original}"
            else:
                return name_original

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to create spelling pronunciation for '{name}': {e}")
            return name

    def _apply_spelling_to_names(self, text: str) -> str:
        """Apply spelling pronunciation to names found in text - DISABLED"""
        # DISABLED: Return text as-is without spelling corrections for names
        return text

        try:
            # Common patterns to identify names in Vietnamese text
            name_indicators = [
                "tÃªn lÃ ", "tÃªn tÃ´i lÃ ", "tÃ´i tÃªn lÃ ",
                "báº¡n tÃªn lÃ ", "cÃ´ áº¥y tÃªn lÃ ", "anh áº¥y tÃªn lÃ ",
                "chá»‹ tÃªn lÃ ", "em tÃªn lÃ ", "bÃ¡c sÄ©", "giÃ¡o viÃªn",
                "cÃ´ giÃ¡o", "tháº§y", "cÃ´", "bÃ ", "Ã´ng", "chÃ¡u",
                "con", "chá»‹", "em", "anh", "chá»‹"
            ]

            result = text
            words = text.split()

            for i, word in enumerate(words):
                # Special handling for single letter spelling patterns
                if len(word) == 1 and word.isupper():
                    # This might be a letter in spelling (P, H, U, C)
                    context_before = ' '.join(words[max(0, i-2):i])
                    context_after = ' '.join(words[i+1:min(len(words), i+3)])

                    # Check if this looks like a spelling pattern
                    is_spelling_pattern = (
                        any(letter in context_before.upper() for letter in ['P', 'H', 'U', 'C', 'B', 'A', 'E', 'I', 'O']) or
                        any(letter in context_after.upper() for letter in ['P', 'H', 'U', 'C', 'B', 'A', 'E', 'I', 'O'])
                    )

                    if is_spelling_pattern:
                        # Convert single letter to proper spelling format
                        spelled_name = f"{word} {word}"
                        result = result.replace(word, spelled_name, 1)
                        logger.debug(f"ğŸ”¤ Single letter spelling: '{word}' â†’ '{spelled_name}'")
                        continue

                # Skip very short words and common Vietnamese words
                if len(word) < 2 or len(word) > 10:
                    continue

                # Check if word looks like a Vietnamese name
                if self._is_likely_vietnamese_name(word):
                    # Check context to see if it's a name
                    context_before = ' '.join(words[max(0, i-3):i])
                    context_after = ' '.join(words[i+1:min(len(words), i+4)])

                    # Check if context suggests this is a name
                    is_name_context = any(indicator in f"{context_before} {context_after}"
                                        for indicator in name_indicators)

                    # More restrictive conditions for adding spelling
                    should_add_spelling = (
                        is_name_context or  # Clear name context
                        (len(word) >= 3 and word[0].isupper() and i > 0) or  # Longer capitalized words after first position
                        self._has_vietnamese_diacritics(word)  # Words with Vietnamese diacritics
                    )

                    if should_add_spelling:
                        spelled_name = self._create_spelling_pronunciation(word)
                        if spelled_name != word:
                            result = result.replace(word, spelled_name, 1)
                            logger.debug(f"ğŸ“ Added spelling to name: '{word}' â†’ '{spelled_name}'")

            return result

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to apply spelling to names: {e}")
            return text

    def _is_likely_vietnamese_name(self, word: str) -> bool:
        """Check if a word is likely a Vietnamese name"""
        try:
            if not word or len(word) < 2:
                return False

            # Vietnamese names typically have these characteristics:
            # 1. Capitalized first letter
            # 2. May contain Vietnamese diacritics
            # 3. Reasonable length (2-10 characters)
            # 4. Not common Vietnamese function words

            if not word[0].isupper():
                return False

            # Common Vietnamese function words to exclude (expanded list)
            function_words = {
                # Basic function words
                'vÃ ', 'cá»§a', 'cÃ¡i', 'lÃ ', 'Ä‘Æ°á»£c', 'cÃ³', 'khÃ´ng',
                'trong', 'trÃªn', 'dÆ°á»›i', 'sang', 'tá»«', 'Ä‘áº¿n',
                'cho', 'vá»›i', 'táº¡i', 'nÃ y', 'Ä‘Ã³', 'mÃ ', 'thÃ¬',
                'nhÆ°ng', 'váº­y', 'sao', 'táº¡i', 'Ä‘Ã¢y', 'kia',

                # Pronouns that are commonly used and shouldn't get spelling
                'tÃ´i', 'báº¡n', 'chá»‹', 'em', 'Ã´ng', 'bÃ ', 'chÃ¡u',
                'anh', 'chá»‹', 'em', 'Ã´ng', 'bÃ ', 'chÃ¡u',
                'nÃ³', 'chÃºng', 'ta', 'tao', 'mÃ¬nh', 'con',

                # Common titles and professions (without specific names)
                'bÃ¡c', 'cÃ´', 'tháº§y', 'bÃ ', 'Ã´ng', 'chá»‹', 'anh', 'em',
                'bÃ¡c sÄ©', 'y tÃ¡', 'giÃ¡o viÃªn', 'há»c sinh', 'sinh viÃªn',
                'cÃ´ng nhÃ¢n', 'ká»¹ sÆ°', 'giÃ¡m Ä‘á»‘c', 'nhÃ¢n viÃªn',

                # Common verbs and adjectives
                'Ä‘i', 'Ä‘áº¿n', 'vá»', 'lÃ m', 'há»c', 'Äƒn', 'uá»‘ng',
                'ngá»§', 'thá»©c', 'Ä‘á»c', 'viáº¿t', 'nÃ³i', 'nghe',
                'xem', 'tháº¥y', 'biáº¿t', 'muá»‘n', 'cáº§n', 'pháº£i',
                'tá»‘t', 'xáº¥u', 'Ä‘áº¹p', 'sáº¡ch', 'báº©n', 'lá»›n', 'nhá»'
            }

            word_lower = word.lower()
            if word_lower in function_words:
                return False

            # Check for Vietnamese diacritics (common in names)
            vietnamese_chars = set('Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘')
            has_diacritics = any(char.lower() in vietnamese_chars for char in word)

            # Vietnamese names often have diacritics or are common name patterns
            return has_diacritics or len(word) >= 3

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to check if likely Vietnamese name: {e}")
            return False

    def _has_vietnamese_diacritics(self, word: str) -> bool:
        """Check if word contains Vietnamese diacritical marks"""
        try:
            vietnamese_diacritics = set('Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘')
            return any(char.lower() in vietnamese_diacritics for char in word)
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to check Vietnamese diacritics: {e}")
            return False

    def _apply_vietnamese_spelling_corrections(self, text: str) -> str:
        """Apply comprehensive Vietnamese spelling corrections with proper tone marks and accents - DISABLED"""
        # DISABLED: Return text as-is without spelling corrections
        return text

        try:
            spelling_corrections = {
                # Basic words with tone marks
                # Words with tone marks (excluding those handled by problematic words)
                "nguoi": "ngÆ°á»i",
                "nhung": "nhÆ°ng",
                "nhieu": "nhiá»u",
                "yeu": "yÃªu",
                "moi": "má»›i",
                "cuoi": "cuá»‘i",
                "trong": "trong",
                "ngoai": "ngoÃ i",
                "tren": "trÃªn",
                "duoi": "dÆ°á»›i",
                "phai": "pháº£i",
                "trai": "trÃ¡i",
                "chan": "chÃ¢n",
                "cuop": "cÆ°á»›p",
                "thay": "thay",
                "doi": "Ä‘á»•i",
                "mua": "mua",
                "an": "Äƒn",
                "uong": "uá»‘ng",
                "xem": "xem",
                "nghe": "nghe",
                "noi": "nÃ³i",
                "viet": "viáº¿t",
                "doc": "Ä‘á»c",
                "hoc": "há»c",
                "lam": "lÃ m",
                "di": "Ä‘i",
                "ve": "vá»",
                "den": "Ä‘áº¿n",
                "tu": "tá»«",
                "voi": "vá»›i",
                "cua": "cá»§a",
                "la": "lÃ ",
                "va": "vÃ ",
                "nhu": "nhÆ°",
                "neu": "náº¿u",
                "khi": "khi",
                "thi": "thÃ¬",
                "ma": "mÃ ",
                "vi": "vÃ¬",

                # Names and proper nouns
                "viet nam": "Viá»‡t Nam",
                "ha noi": "HÃ  Ná»™i",
                "ho chi minh": "Há»“ ChÃ­ Minh",
                "da nang": "ÄÃ  Náºµng",
                "hai phong": "Háº£i PhÃ²ng",
                "can tho": "Cáº§n ThÆ¡",

                # Time expressions with correct spelling
                "hom nay": "hÃ´m nay",
                "hom qua": "hÃ´m qua",
                "hom kia": "hÃ´m kia",
                "sang nay": "sÃ¡ng nay",
                "chieu nay": "chiá»u nay",
                "toi nay": "tá»‘i nay",
                "sang som": "sÃ¡ng sá»›m",
                "chieu muon": "chiá»u muá»™n",

                # Question words with proper spelling
                "gi": "gÃ¬",
                "tai sao": "táº¡i sao",
                "o dau": "á»Ÿ Ä‘Ã¢u",
                "tu dau": "tá»« Ä‘Ã¢u",
                "den dau": "Ä‘áº¿n Ä‘Ã¢u",
                "bao gio": "bao giá»",
                "khi nao": "khi nÃ o",
                "the nao": "tháº¿ nÃ o",
                "nhu the nao": "nhÆ° tháº¿ nÃ o",

                # Common phrases with correct spelling (excluding problematic ones)
                "cam on": "cáº£m Æ¡n",
                "xin loi": "xin lá»—i",
                "khong sao": "khÃ´ng sao",
                "duoc roi": "Ä‘Æ°á»£c rá»“i",
                "tam biet": "táº¡m biá»‡t",
                "chao ban": "chÃ o báº¡n",
                "co le": "cÃ³ láº½",
                "chac chan": "cháº¯c cháº¯n",
                "can than": "cáº©n tháº­n",
                "nhanh len": "nhanh lÃªn",
                "doi chut": "Ä‘á»£i chÃºt",

                # Numbers with proper spelling
                "mot": "má»™t",
                "hai": "hai",
                "ba": "ba",
                "bon": "bá»‘n",
                "nam": "nÄƒm",
                "sau": "sÃ¡u",
                "bay": "báº£y",
                "tam": "tÃ¡m",
                "chin": "chÃ­n",
                "muoi": "mÆ°á»i",
                "tram": "trÄƒm",
                "nghin": "nghÃ¬n",
                "trieu": "triá»‡u",
                "ty": "tá»·",

                # Family relationships
                "bo me": "bá»‘ máº¹",
                "cha me": "cha máº¹",
                "ong ba": "Ã´ng bÃ ",
                "co di": "cÃ´ dÃ¬",
                "chu bac": "chÃº bÃ¡c",
                "anh chi": "anh chá»‹",
                "em gai": "em gÃ¡i",
                "em trai": "em trai",

                # Colors with proper spelling
                "do": "Ä‘á»",
                "xanh": "xanh",
                "vang": "vÃ ng",
                "trang": "tráº¯ng",
                "den": "Ä‘en",
                "tim": "tÃ­m",
                "nau": "nÃ¢u",
                "xam": "xÃ¡m",

                # Days of the week with correct accents
                "thu hai": "thá»© hai",
                "thu ba": "thá»© ba",
                "thu tu": "thá»© tÆ°",
                "thu nam": "thá»© nÄƒm",
                "thu sau": "thá»© sÃ¡u",
                "thu bay": "thá»© báº£y",
                "chu nhat": "chá»§ nháº­t",

                # Months with correct accents
                "thang mot": "thÃ¡ng má»™t",
                "thang hai": "thÃ¡ng hai",
                "thang ba": "thÃ¡ng ba",
                "thang tu": "thÃ¡ng tÆ°",
                "thang nam": "thÃ¡ng nÄƒm",
                "thang sau": "thÃ¡ng sÃ¡u",
                "thang bay": "thÃ¡ng báº£y",
                "thang tam": "thÃ¡ng tÃ¡m",
                "thang chin": "thÃ¡ng chÃ­n",
                "thang muoi": "thÃ¡ng mÆ°á»i",
                "thang muoi mot": "thÃ¡ng mÆ°á»i má»™t",
                "thang muoi hai": "thÃ¡ng mÆ°á»i hai",

                # Advanced spelling corrections with multiple syllables
                "thay doi": "thay Ä‘á»•i",
                "cam thay": "cáº£m tháº¥y",
                "cam on": "cáº£m Æ¡n",
                "xin loi": "xin lá»—i",
                "tam biet": "táº¡m biá»‡t",
                "chao ban": "chÃ o báº¡n",
                "rat vui": "ráº¥t vui",
                "co le": "cÃ³ láº½",
                "chac chan": "cháº¯c cháº¯n",
                "co the": "cÃ³ thá»ƒ",
                "khong the": "khÃ´ng thá»ƒ",
                "can than": "cáº©n tháº­n",
                "nhanh len": "nhanh lÃªn",
                "doi chut": "Ä‘á»£i chÃºt",

                # Cognitive assessment specific spelling
                "ban ten gi": "báº¡n tÃªn gÃ¬",
                "ban bao nhieu tuoi": "báº¡n bao nhiÃªu tuá»•i",
                "ban dang o dau": "báº¡n Ä‘ang á»Ÿ Ä‘Ã¢u",
                "hom nay la thu may": "hÃ´m nay lÃ  thá»© máº¥y",
                "ban nam tuoi": "báº¡n nÄƒm tuá»•i",
                "ban muoi tuoi": "báº¡n mÆ°á»i tuá»•i",
                "ban hai muoi tuoi": "báº¡n hai mÆ°Æ¡i tuá»•i",

                # Medical terms with correct spelling
                "benh vien": "bá»‡nh viá»‡n",
                "bac si": "bÃ¡c sÄ©",
                "y ta": "y tÃ¡",
                "duoc si": "dÆ°á»£c sÄ©",
                "thuoc": "thuá»‘c",
                "vien thuoc": "viÃªn thuá»‘c",
                "uong thuoc": "uá»‘ng thuá»‘c",
                "dau dau": "Ä‘au Ä‘áº§u",
                "dau bung": "Ä‘au bá»¥ng",
                "dau nguc": "Ä‘au ngá»±c",
                "dau tay": "Ä‘au tay",
                "dau chan": "Ä‘au chÃ¢n",
                "dau rang": "Ä‘au rÄƒng",
            }

            for incorrect, correct in spelling_corrections.items():
                text = text.replace(incorrect, correct)

            return text

        except Exception as e:
            # DISABLED: Vietnamese spelling corrections error logging
            return text

    def _apply_specialized_corrections(self, text: str) -> str:
        """Apply specialized corrections for complex cases"""
        try:
            specialized_corrections = {
                # Numbers in context
                "hai nghin": "hai nghÃ¬n",
                "chin tram": "chÃ­n trÄƒm",
                "chin muoi": "chÃ­n mÆ°Æ¡i",
                "muoi chin": "mÆ°á»i chÃ­n",

                # Medical terms
                "benh vien": "bá»‡nh viá»‡n",
                "bac si": "bÃ¡c sÄ©",
                "thuoc": "thuá»‘c",

                # Complex phrases
                "la ban": "lÃ  báº¡n",
                "nam tuoi": "nÄƒm tuá»•i",
                "hom nay": "hÃ´m nay",
                "la thu": "lÃ  thá»©",
                "thang mot": "thÃ¡ng má»™t",
                "nam hai": "nÄƒm hai",
                "nghin chin": "nghÃ¬n chÃ­n",
                "tram chin": "trÄƒm chÃ­n",
                "muoi chin": "mÆ°á»i chÃ­n",
                "bao nhieu": "bao nhiÃªu",
                "la thu may": "lÃ  thá»© máº¥y",
                "dang o dau": "Ä‘ang á»Ÿ Ä‘Ã¢u",

                # Regional expressions that need multiple corrections
                "chung tao": "chÃºng tÃ´i",
                "chung tao": "chÃºng tÃ´i",  # Apply twice for better results
                "biet khong": "biáº¿t khÃ´ng",
                "tao la": "tÃ´i lÃ ",

                # Numbers in sentences
                "mot nam": "má»™t nÄƒm",
                "hai nam": "hai nÄƒm",
                "ba nam": "ba nÄƒm",
                "bon nam": "bá»‘n nÄƒm",
                "nam nam": "nÄƒm nÄƒm",

                # Age expressions
                "tuoi toi": "tuá»•i tÃ´i",
                "tuoi ban": "tuá»•i báº¡n",

                # Medical terms corrections
                "bÃ©nh vien": "bá»‡nh viá»‡n",
                "bÃ c si": "bÃ¡c sÄ©",
                "thuÃ³c": "thuá»‘c",

                # Common phrase corrections
                "ten gi": "tÃªn gÃ¬",
                "o dau": "á»Ÿ Ä‘Ã¢u",
                "lam gi": "lÃ m gÃ¬",
                "bÃ o nhiá»ƒu": "bao nhiÃªu",
                "dang o Ä‘au": "Ä‘ang á»Ÿ Ä‘Ã¢u",
                "vi Ä‘au Ä‘au": "vÃ¬ Ä‘au Ä‘áº§u",

                # Final pronunciation fixes
                "tui la": "tÃ´i lÃ ",
                "chung tui": "chÃºng tÃ´i",
                "nghÄ©n chá»‹n": "nghÃ¬n chÃ­n",
                "tram chá»‹n": "trÄƒm chÃ­n",
                "mÆ°á»i chá»‹n": "mÆ°á»i chÃ­n",

                # Cognitive assessment specific
                "ban ten gi": "báº¡n tÃªn gÃ¬",
                "ban bao nhieu tuoi": "báº¡n bao nhiÃªu tuá»•i",
                "ban dang o dau": "báº¡n Ä‘ang á»Ÿ Ä‘Ã¢u",
                "hom nay la thu may": "hÃ´m nay lÃ  thá»© máº¥y",

                # Time expressions
                "hom qua": "hÃ´m qua",
                "hom kia": "hÃ´m kia",
                "tuan truoc": "tuáº§n trÆ°á»›c",
                "thang truoc": "thÃ¡ng trÆ°á»›c",
                "nam truoc": "nÄƒm trÆ°á»›c",
            }

            for incorrect, correct in specialized_corrections.items():
                text = text.replace(incorrect, correct)

            return text

        except Exception as e:
            # DISABLED: Specialized corrections error logging
            return text

    def _apply_tone_accent_corrections(self, text: str) -> str:
        """Apply Vietnamese tone/accent corrections for better audio processing"""
        try:
            tone_corrections = {
                # Common tone misrecognition patterns
                "nghien": "nghiá»‡n",  # sáº¯c â†’ ngang (addiction)
                "nghien cuu": "nghiÃªn cá»©u",  # research
                "nghien": "nghiá»n",  # crush/grind
                "thuong": "thÆ°Æ¡ng",  # thÆ°Æ¡ng (love/pity)
                "thuong": "thÆ°á»ng",  # thÆ°á»ng (usually)
                "thuong": "thÆ°á»Ÿng",  # thÆ°á»Ÿng (reward)
                "thuong": "thÆ°á»£ng",  # thÆ°á»£ng (upper)
                "thuong": "thÆ°á»Ÿng",  # thÆ°á»Ÿng (reward)

                # Medical terms tone corrections
                "dau dau": "Ä‘au Ä‘áº§u",  # headache
                "dau bung": "Ä‘au bá»¥ng",  # stomachache
                "dau nguc": "Ä‘au ngá»±c",  # chest pain
                "dau tay": "Ä‘au tay",  # hand pain
                "dau chan": "Ä‘au chÃ¢n",  # foot pain

                # Cognitive assessment terms
                "nho": "nhá»›",  # remember
                "quen": "quÃªn",  # forget
                "biet": "biáº¿t",  # know
                "hieu": "hiá»ƒu",  # understand
                "muon": "muá»‘n",  # want
                "can": "cáº§n",  # need
                "phai": "pháº£i",  # must

                # Common words with tone variations
                "di": "Ä‘i",  # go
                "di": "dá»‹",  # strange (rare)
                "ma": "mÃ ",  # but
                "ma": "mÃ£",  # code
                "ma": "mÃ¡",  # mother
                "ba": "ba",  # father
                "ba": "bÃ¡",  # uncle
                "ba": "bÃ ",  # grandmother
                "me": "máº¹",  # mother
                "me": "máº¿",  # drunk (slang)
            }

            for incorrect, correct in tone_corrections.items():
                text = text.replace(incorrect, correct)

            return text

        except Exception as e:
            # DISABLED: Tone/accent corrections error logging
            return text

    def _apply_semantic_context_corrections(self, text: str) -> str:
        """Apply semantic and context-based corrections for Vietnamese"""
        try:
            # Context-based corrections for medical/cognitive terms
            semantic_corrections = {
                # Medical context
                "benh nhan": "bá»‡nh nhÃ¢n",  # patient
                "bac si": "bÃ¡c sÄ©",  # doctor
                "y ta": "y tÃ¡",  # nurse
                "thuoc": "thuá»‘c",  # medicine
                "kham benh": "khÃ¡m bá»‡nh",  # medical examination
                "uá»‘ng thuá»‘c": "uá»‘ng thuá»‘c",  # take medicine
                "Ä‘au Ä‘áº§u": "Ä‘au Ä‘áº§u",  # headache
                "Ä‘au bá»¥ng": "Ä‘au bá»¥ng",  # stomachache

                # Cognitive assessment context
                "nhá»›": "nhá»›",  # remember
                "quÃªn": "quÃªn",  # forget
                "biáº¿t": "biáº¿t",  # know
                "hiá»ƒu": "hiá»ƒu",  # understand
                "táº­p trung": "táº­p trung",  # concentrate
                "chÃº Ã½": "chÃº Ã½",  # pay attention

                # Time context
                "hÃ´m nay": "hÃ´m nay",  # today
                "hÃ´m qua": "hÃ´m qua",  # yesterday
                "hÃ´m kia": "hÃ´m kia",  # day before yesterday
                "tuáº§n nÃ y": "tuáº§n nÃ y",  # this week
                "thÃ¡ng nÃ y": "thÃ¡ng nÃ y",  # this month

                # Age context
                "tuá»•i": "tuá»•i",  # age
                "sinh nháº­t": "sinh nháº­t",  # birthday
                "lá»›n tuá»•i": "lá»›n tuá»•i",  # elderly

                # Common phrases
                "tÃ´i tÃªn lÃ ": "tÃ´i tÃªn lÃ ",  # my name is
                "báº¡n tÃªn lÃ ": "báº¡n tÃªn lÃ ",  # your name is
                "bao nhiÃªu tuá»•i": "bao nhiÃªu tuá»•i",  # how old
                "á»Ÿ Ä‘Ã¢u": "á»Ÿ Ä‘Ã¢u",  # where
                "lÃ m gÃ¬": "lÃ m gÃ¬",  # what do
            }

            for incorrect, correct in semantic_corrections.items():
                text = text.replace(incorrect, correct)

            return text

        except Exception as e:
            # DISABLED: Semantic context corrections error logging
            return text

    def _apply_phonological_corrections(self, text: str) -> str:
        """Apply advanced phonological corrections for Vietnamese speech patterns"""
        try:
            phonological_corrections = {
                # Consonant cluster corrections (common ASR errors)
                "khong": "khÃ´ng",  # not
                "nguoi": "ngÆ°á»i",  # person
                "nghe": "nghe",  # hear
                "nghi": "nghÄ©",  # think
                "ngay": "ngÃ y",  # day
                "trong": "trong",  # in/inside
                "thanh": "thÃ nh",  # become
                "thanh": "thanh",  # voice/sound
                "thanh": "tháº¯ng",  # win

                # Vowel sequence corrections
                "uong": "uá»‘ng",  # drink
                "uong": "Æ°Æ¡ng",  # related to mother
                "ien": "iÃªn",  # related to connection
                "ien": "iá»‡n",  # related to electricity
                "ien": "iá»…n",  # related to far

                # Dipthong corrections
                "ai": "ai",  # who
                "ao": "ao",  # pond
                "au": "au",  # oh
                "ay": "ay",  # here
                "eo": "eo",  # narrow
                "eu": "eu",  # oh (southern)
                "ia": "ia",  # related to mother
                "ieu": "iá»u",  # willow
                "iu": "iu",  # oh
                "oa": "oa",  # related to grandmother
                "oai": "oai",  # majestic
                "oe": "oe",  # related to sister
                "oi": "oi",  # oh
                "oo": "oo",  # oh (dialect)
                "ua": "ua",  # fall
                "uai": "uai",  # dialect variation
                "ue": "ue",  # dialect variation
                "ui": "ui",  # ash
                "uo": "uo",  # dialect variation
                "uu": "uu",  # dialect variation
                "uy": "uy",  # dialect variation

                # Common assimilation errors
                "cua": "cá»§a",  # of
                "voi": "vá»›i",  # with
                "tu": "tá»«",  # from
                "den": "Ä‘áº¿n",  # to/arrive
                "ve": "vá»",  # return
                "di": "Ä‘i",  # go
                "lai": "láº¡i",  # again
                "nhau": "nhau",  # each other

                # Medical phonological patterns
                "kham": "khÃ¡m",  # examine
                "benh": "bá»‡nh",  # disease
                "vien": "viá»‡n",  # institute
                "vien": "viÃªn",  # pill/round
                "uong": "uá»‘ng",  # drink
                "an": "Äƒn",  # eat
                "ngu": "ngá»§",  # sleep

                # Cognitive phonological patterns
                "nho": "nhá»›",  # remember
                "quen": "quÃªn",  # forget
                "biet": "biáº¿t",  # know
                "hieu": "hiá»ƒu",  # understand
                "muon": "muá»‘n",  # want
                "can": "cáº§n",  # need
                "phai": "pháº£i",  # must/correct
                "dung": "Ä‘Ãºng",  # correct
                "sai": "sai",  # wrong
            }

            for incorrect, correct in phonological_corrections.items():
                text = text.replace(incorrect, correct)

            return text

        except Exception as e:
            # DISABLED: Phonological corrections error logging
            return text

    def _apply_vietnamese_specific_corrections(self, text: str) -> str:
        """Apply comprehensive Vietnamese-specific corrections and improvements with enhanced audio processing"""
        if not text:
            return text

        try:
            corrected_text = text.lower()

            # Step 1: Apply tone/accent corrections (Æ°u tiÃªn cao nháº¥t)
            corrected_text = self._apply_tone_accent_corrections(corrected_text)

            # Step 2: Apply pronunciation-based corrections (Ä‘Ã¡nh váº§n)
            corrected_text = self._apply_pronunciation_corrections(corrected_text)

            # Step 3: Apply regional/dialect corrections (ngÃ´n ngá»¯ Ä‘á»‹a phÆ°Æ¡ng)
            corrected_text = self._apply_regional_corrections(corrected_text)

            # Step 4: Apply semantic/context corrections (ngá»¯ nghÄ©a)
            corrected_text = self._apply_semantic_context_corrections(corrected_text)

            # Step 5: Apply common ASR error corrections
            corrected_text = self._apply_common_asr_corrections(corrected_text)

            # Step 6: Apply advanced phonological corrections
            corrected_text = self._apply_phonological_corrections(corrected_text)

            # Step 7: Apply spelling and grammatical corrections
            corrected_text = self._apply_spelling_corrections(corrected_text)

            # Step 8: Apply context-aware improvements
            corrected_text = self._apply_context_improvements(corrected_text)

            # Step 9: Final formatting and capitalization
            corrected_text = self._apply_final_formatting(corrected_text, text)

            return corrected_text

        except Exception as e:
            # DISABLED: Vietnamese-specific corrections error logging
            return text

    def _apply_pronunciation_corrections(self, text: str) -> str:
        """Correct common pronunciation-based errors in Vietnamese"""
        pronunciation_corrections = {
            # Southern Vietnamese pronunciation
            "thÃ¬": ["thi", "thÃ¬", "thÃ¬"],
            "tháº¿": ["the", "tháº¿", "tháº¿"],
            "tháº¿ nÃ o": ["the nao", "tháº¿ nÃ o", "tháº¿ nÃ o"],
            "tháº¿ mÃ ": ["the ma", "tháº¿ mÃ ", "tháº¿ mÃ "],

            # Northern Vietnamese pronunciation
            "khÃ´ng": ["khong", "khÃ´ng", "khÃ´ng"],
            "ngÆ°á»i": ["nguoi", "ngÆ°á»i", "ngÆ°á»i"],
            "chÃºng tÃ´i": ["chung toi", "chÃºng tÃ´i", "chÃºng tÃ´i"],
            "chÃºng ta": ["chung ta", "chÃºng ta", "chÃºng ta"],

            # Common mispronunciations
            "tÃ´i": ["toi", "tÃ´i", "tÃ´i"],
            "báº¡n": ["ban", "báº¡n", "báº¡n"],
            "anh": ["anh", "anh", "anh"],
            "chá»‹": ["chi", "chá»‹", "chá»‹"],
            "em": ["em", "em", "em"],
            "Ã´ng": ["ong", "Ã´ng", "Ã´ng"],
            "bÃ ": ["ba", "bÃ ", "bÃ "],
            "cáº­u": ["cau", "cáº­u", "cáº­u"],
            "bÃ©": ["be", "bÃ©", "bÃ©"],

            # Numbers and quantities (very important for cognitive assessment)
            "má»™t": ["mot", "má»™t", "má»™t"],
            "hai": ["hai", "hai", "hai"],
            "ba": ["ba", "ba", "ba"],
            "bá»‘n": ["bon", "bá»‘n", "bá»‘n"],
            "nÄƒm": ["nam", "nÄƒm", "nÄƒm"],
            "sÃ¡u": ["sau", "sÃ¡u", "sÃ¡u"],
            "báº£y": ["bay", "báº£y", "báº£y"],
            "tÃ¡m": ["tam", "tÃ¡m", "tÃ¡m"],
            "chÃ­n": ["chin", "chÃ­n", "chÃ­n"],
            "mÆ°á»i": ["muoi", "mÆ°á»i", "mÆ°á»i"],
            "hai mÆ°Æ¡i": ["hai muoi", "hai mÆ°Æ¡i", "hai mÆ°Æ¡i"],
            "ba mÆ°Æ¡i": ["ba muoi", "ba mÆ°Æ¡i", "ba mÆ°Æ¡i"],

            # Days of the week (critical for cognitive assessment)
            "thá»© hai": ["thu hai", "thá»© hai", "thá»© hai"],
            "thá»© ba": ["thu ba", "thá»© ba", "thá»© ba"],
            "thá»© tÆ°": ["thu tu", "thá»© tÆ°", "thá»© tÆ°"],
            "thá»© nÄƒm": ["thu nam", "thá»© nÄƒm", "thá»© nÄƒm"],
            "thá»© sÃ¡u": ["thu sau", "thá»© sÃ¡u", "thá»© sÃ¡u"],
            "thá»© báº£y": ["thu bay", "thá»© báº£y", "thá»© báº£y"],
            "chá»§ nháº­t": ["chu nhat", "chá»§ nháº­t", "chá»§ nháº­t"],

            # Months (important for orientation assessment)
            "thÃ¡ng má»™t": ["thang mot", "thÃ¡ng má»™t", "thÃ¡ng má»™t"],
            "thÃ¡ng hai": ["thang hai", "thÃ¡ng hai", "thÃ¡ng hai"],
            "thÃ¡ng ba": ["thang ba", "thÃ¡ng ba", "thÃ¡ng ba"],
            "thÃ¡ng tÆ°": ["thang tu", "thÃ¡ng tÆ°", "thÃ¡ng tÆ°"],
            "thÃ¡ng nÄƒm": ["thang nam", "thÃ¡ng nÄƒm", "thÃ¡ng nÄƒm"],
            "thÃ¡ng sÃ¡u": ["thang sau", "thÃ¡ng sÃ¡u", "thÃ¡ng sÃ¡u"],
            "thÃ¡ng báº£y": ["thang bay", "thÃ¡ng báº£y", "thÃ¡ng báº£y"],
            "thÃ¡ng tÃ¡m": ["thang tam", "thÃ¡ng tÃ¡m", "thÃ¡ng tÃ¡m"],
            "thÃ¡ng chÃ­n": ["thang chin", "thÃ¡ng chÃ­n", "thÃ¡ng chÃ­n"],
            "thÃ¡ng mÆ°á»i": ["thang muoi", "thÃ¡ng mÆ°á»i", "thÃ¡ng mÆ°á»i"],
            "thÃ¡ng mÆ°á»i má»™t": ["thang muoi mot", "thÃ¡ng mÆ°á»i má»™t", "thÃ¡ng mÆ°á»i má»™t"],
            "thÃ¡ng mÆ°á»i hai": ["thang muoi hai", "thÃ¡ng mÆ°á»i hai", "thÃ¡ng mÆ°á»i hai"],
        }

        for correct, variations in pronunciation_corrections.items():
            for variation in variations:
                text = text.replace(variation, correct)

        return text

    def _apply_regional_corrections(self, text: str) -> str:
        """Correct regional/dialect variations in Vietnamese"""
        regional_corrections = {
            # Southern dialect
            "thÃ¬": ["thi", "thÃ¬"],
            "tháº¿": ["the", "tháº¿"],
            "chá»‹": ["chi", "chá»‹"],
            "anh": ["anh", "anh"],
            "em": ["em", "em"],
            "tÃ´i": ["tui", "tÃ´i"],  # Southern "tui" â†’ "tÃ´i"
            "tui": ["tÃ´i", "tÃ´i"],  # Southern "tui" â†’ "tÃ´i"
            "mÃ¬nh": ["mih", "mÃ¬nh"],

            # Northern dialect
            "khÃ´ng": ["khÃ´ng", "khÃ´ng"],
            "ngÆ°á»i": ["ngÆ°á»i", "ngÆ°á»i"],
            "chÃºng tÃ´i": ["bá»n tÃ´i", "chÃºng tÃ´i"],
            "chÃºng tao": ["chÃºng tÃ´i", "chÃºng tÃ´i"],
            "tao": ["tÃ´i", "tÃ´i"],  # Northern "tao" â†’ "tÃ´i"

            # Central dialect
            "chá»‹": ["cÃ´", "chá»‹"],  # Central sometimes use "cÃ´" for "chá»‹"
            "anh": ["Ã´ng", "anh"],  # Central sometimes use "Ã´ng" for "anh"

            # Common regional expressions
            "Ä‘i chÆ¡i": ["Ä‘i chÆ¡i", "Ä‘i chÆ¡i"],
            "Äƒn cÆ¡m": ["Äƒn cÆ¡m", "Äƒn cÆ¡m"],
            "uá»‘ng nÆ°á»›c": ["uá»‘ng nÆ°á»›c", "uá»‘ng nÆ°á»›c"],
        }

        for correct, variations in regional_corrections.items():
            for variation in variations:
                text = text.replace(variation, correct)

        return text

    def _apply_common_asr_corrections(self, text: str) -> str:
        """Correct common ASR (Automatic Speech Recognition) errors"""
        asr_corrections = {
            # Common ASR misrecognitions
            "cÃ³ thá»ƒ": ["cÃ³ thá»ƒ", "cÃ³ tháº¿", "cÃ³ thá»ƒ"],
            "khÃ´ng thá»ƒ": ["khÃ´ng thá»ƒ", "khÃ´ng tháº¿", "khÃ´ng thá»ƒ"],
            "Ä‘Æ°á»£c": ["Ä‘Æ°á»£c", "Ä‘Æ°á»£c", "Ä‘Æ°á»£c"],
            "nhÆ°ng": ["nhÆ°ng", "nhÆ°ng", "nhÆ°ng"],
            "nhiá»u": ["nhiá»u", "nhiá»u", "nhiá»u"],
            "báº¡n": ["báº¡n", "báº¡n", "báº¡n"],
            "tÃ´i": ["tÃ´i", "tÃ´i", "tÃ´i"],
            "cáº£m Æ¡n": ["cáº£m Æ¡n", "cáº£m Æ¡n", "cáº£m Æ¡n"],
            "xin chÃ o": ["xin chÃ o", "xin chÃ o", "xin chÃ o"],
            "tá»‘t": ["tá»‘t", "tá»‘t", "tá»‘t"],
            "xáº¥u": ["xáº¥u", "xáº¥u", "xáº¥u"],
            "nhÃ ": ["nhÃ ", "nhÃ ", "nhÃ "],
            "trÆ°á»ng": ["trÆ°á»ng", "trÆ°á»ng", "trÆ°á»ng"],
            "há»c": ["há»c", "há»c", "há»c"],
            "Ä‘i": ["Ä‘i", "Ä‘i", "Ä‘i"],
            "vá»": ["vá»", "vá»", "vá»"],
            "Äƒn": ["Äƒn", "Äƒn", "Äƒn"],
            "uá»‘ng": ["uá»‘ng", "uá»‘ng", "uá»‘ng"],
            "ngá»§": ["ngá»§", "ngá»§", "ngá»§"],
            "lÃ m": ["lÃ m", "lÃ m", "lÃ m"],
            "viá»‡c": ["viá»‡c", "viá»‡c", "viá»‡c"],

            # Medical/cognitive assessment specific terms
            "nhá»›": ["nhá»›", "nhá»›", "nhá»›"],
            "quÃªn": ["quÃªn", "quÃªn", "quÃªn"],
            "hÃ´m nay": ["hÃ´m nay", "hÃ´m nay", "hÃ´m nay"],
            "hÃ´m qua": ["hÃ´m qua", "hÃ´m qua", "hÃ´m qua"],
            "tuáº§n trÆ°á»›c": ["tuáº§n trÆ°á»›c", "tuáº§n trÆ°á»›c", "tuáº§n trÆ°á»›c"],
            "thÃ¡ng trÆ°á»›c": ["thÃ¡ng trÆ°á»›c", "thÃ¡ng trÆ°á»›c", "thÃ¡ng trÆ°á»›c"],
            "nÄƒm trÆ°á»›c": ["nÄƒm trÆ°á»›c", "nÄƒm trÆ°á»›c", "nÄƒm trÆ°á»›c"],
            "bá»‡nh viá»‡n": ["bá»‡nh viá»‡n", "bá»‡nh viá»‡n", "bá»‡nh viá»‡n"],
            "bÃ¡c sÄ©": ["bÃ¡c sÄ©", "bÃ¡c sÄ©", "bÃ¡c sÄ©"],
            "thuá»‘c": ["thuá»‘c", "thuá»‘c", "thuá»‘c"],
            "Ä‘au": ["Ä‘au", "Ä‘au", "Ä‘au"],
            "má»‡t": ["má»‡t", "má»‡t", "má»‡t"],
            "ngá»§": ["ngá»§", "ngá»§", "ngá»§"],
            "Äƒn": ["Äƒn", "Äƒn", "Äƒn"],
            "uá»‘ng": ["uá»‘ng", "uá»‘ng", "uá»‘ng"],

            # Cognitive assessment specific phrases (prioritize these)
            "báº¡n tÃªn gÃ¬": ["ban ten gi", "báº¡n tÃªn gÃ¬", "báº¡n tÃªn gÃ¬"],
            "báº¡n bao nhiÃªu tuá»•i": ["ban bao nhieu tuoi", "báº¡n bao nhiÃªu tuá»•i", "báº¡n bao nhiÃªu tuá»•i"],
            "hÃ´m nay lÃ  thá»© máº¥y": ["hom nay la thu may", "hÃ´m nay lÃ  thá»© máº¥y", "hÃ´m nay lÃ  thá»© máº¥y"],
            "báº¡n Ä‘ang á»Ÿ Ä‘Ã¢u": ["ban dang o dau", "báº¡n Ä‘ang á»Ÿ Ä‘Ã¢u", "báº¡n Ä‘ang á»Ÿ Ä‘Ã¢u"],

            # Additional cognitive assessment terms
            "tuá»•i": ["tuoi", "tuá»•i", "tuá»•i"],
            "sinh nháº­t": ["sinh nhat", "sinh nháº­t", "sinh nháº­t"],
            "bá»‡nh viá»‡n": ["benh vien", "bá»‡nh viá»‡n", "bá»‡nh viá»‡n"],
            "bÃ¡c sÄ©": ["bac si", "bÃ¡c sÄ©", "bÃ¡c sÄ©"],
            "thuá»‘c": ["thuoc", "thuá»‘c", "thuá»‘c"],
            "Ä‘au ": ["dau ", "Ä‘au ", "Ä‘au "],  # Add space to distinguish from "Ä‘áº§u"
            "nhá»›": ["nho", "nhá»›", "nhá»›"],
            "quÃªn": ["quen", "quÃªn", "quÃªn"],

            # Question words
            "tÃªn gÃ¬": ["ten gi", "tÃªn gÃ¬", "tÃªn gÃ¬"],
            "á»Ÿ Ä‘Ã¢u": ["o dau", "á»Ÿ Ä‘Ã¢u", "á»Ÿ Ä‘Ã¢u"],
            "lÃ m gÃ¬": ["lam gi", "lÃ m gÃ¬", "lÃ m gÃ¬"],
            "bao nhiÃªu": ["bao nhieu", "bao nhiÃªu", "bao nhiÃªu"],
            "thá»© máº¥y": ["thu may", "thá»© máº¥y", "thá»© máº¥y"],

            # Context-specific corrections to avoid conflicts
            "báº¡n ": ["ban ", "báº¡n ", "báº¡n "],  # Prioritize "báº¡n" over "bÃ¡n"
            "tÃ´i ": ["toi ", "tÃ´i ", "tÃ´i "],  # Ensure correct pronoun
        }

        for correct, variations in asr_corrections.items():
            for variation in variations:
                text = text.replace(variation, correct)

        return text

    def _apply_spelling_corrections(self, text: str) -> str:
        """Apply Vietnamese spelling corrections for ASR errors - DISABLED"""
        # DISABLED: Return text as-is without spelling corrections
        return text

        # This method now focuses on ASR-specific spelling corrections
        # The comprehensive spelling corrections are handled by _apply_vietnamese_spelling_corrections
        spelling_corrections = {
            # Common ASR misspellings that need special handling
            "nghÄ©": ["nghi", "nghÄ©"],
            "nghe": ["nghe", "nghe"],
            "viáº¿t": ["viet", "viáº¿t"],
            "Ä‘á»c": ["doc", "Ä‘á»c"],
            "biáº¿t": ["biet", "biáº¿t"],
            "muá»‘n": ["muon", "muá»‘n"],
            "hiá»ƒu": ["hieu", "hiá»ƒu"],
            "há»i": ["hoi", "há»i"],
            "tháº¥y": ["thay", "tháº¥y"],
            "tráº£ lá»i": ["tra loi", "tráº£ lá»i"],
            "cÃ¢u há»i": ["cau hoi", "cÃ¢u há»i"],
            "Ä‘Ã¡p Ã¡n": ["dap an", "Ä‘Ã¡p Ã¡n"],

            # Age-related terms (important for cognitive assessment)
            "tuá»•i": ["tuoi", "tuá»•i"],
            "sinh nháº­t": ["sinh nhat", "sinh nháº­t"],
            "tuá»•i tÃ¡c": ["tuoi tac", "tuá»•i tÃ¡c"],
            "giÃ ": ["gia", "giÃ "],
            "tráº»": ["tre", "tráº»"],
            "lá»›n tuá»•i": ["lon tuoi", "lá»›n tuá»•i"],

            # Time-related terms
            "giá»": ["gio", "giá»"],
            "phÃºt": ["phut", "phÃºt"],
            "giÃ¢y": ["giay", "giÃ¢y"],
            "ngÃ y": ["ngay", "ngÃ y"],
            "thÃ¡ng": ["thang", "thÃ¡ng"],
            "nÄƒm": ["nam", "nÄƒm"],
            "tuáº§n": ["tuan", "tuáº§n"],
        }

        for correct, variations in spelling_corrections.items():
            for variation in variations:
                text = text.replace(variation, correct)

        return text

    def _apply_context_improvements(self, text: str) -> str:
        """Apply context-aware improvements for Vietnamese text"""
        # Improve sentence structure and flow
        improvements = {
            "tÃ´i lÃ ": ["tÃ´i lÃ ", "tÃ´i lÃ "],
            "tÃ´i cÃ³": ["tÃ´i cÃ³", "tÃ´i cÃ³"],
            "tÃ´i Ä‘Ã£": ["tÃ´i Ä‘Ã£", "tÃ´i Ä‘Ã£"],
            "tÃ´i sáº½": ["tÃ´i sáº½", "tÃ´i sáº½"],
            "tÃ´i muá»‘n": ["tÃ´i muá»‘n", "tÃ´i muá»‘n"],
            "tÃ´i cáº§n": ["tÃ´i cáº§n", "tÃ´i cáº§n"],
        }

        for correct, variations in improvements.items():
            for variation in variations:
                text = text.replace(variation, correct)

        return text

    def _apply_final_formatting(self, text: str, original_text: str) -> str:
        """Apply final formatting and capitalization"""
        # Split into sentences and capitalize properly
        sentences = text.split('. ')
        capitalized_sentences = []

        for sentence in sentences:
            if sentence.strip():
                sentence = sentence.strip()
                # Capitalize first letter
                if sentence and sentence[0].islower():
                    sentence = sentence[0].upper() + sentence[1:]

                # Capitalize "TÃ´i" at start of sentence
                if sentence.lower().startswith('tÃ´i'):
                    sentence = 'TÃ´i' + sentence[3:]

                # Capitalize other pronouns at start
                pronouns = ['báº¡n', 'anh', 'chá»‹', 'em', 'Ã´ng', 'bÃ ', 'chÃ¡u', 'cáº­u', 'bÃ©']
                for pronoun in pronouns:
                    if sentence.lower().startswith(pronoun + ' '):
                        sentence = pronoun.capitalize() + sentence[len(pronoun):]
                        break

                capitalized_sentences.append(sentence)

        result = '. '.join(capitalized_sentences)

        # Ensure result ends with proper punctuation if original had it
        if original_text.endswith('.') and not result.endswith('.'):
            result += '.'
        elif original_text.endswith('!') and not result.endswith('!'):
            result += '!'
        elif original_text.endswith('?') and not result.endswith('?'):
            result += '?'

        return result

    def _apply_english_specific_corrections(self, text: str) -> str:
        """Apply English-specific corrections and improvements"""
        if not text:
            return text

        try:
            # English common corrections
            english_corrections = {
                # Common ASR errors for English
                "i": "I",  # Capitalize "I"
                "im": "I'm",
                "dont": "don't",
                "cant": "can't",
                "wont": "won't",
                "shouldnt": "shouldn't",
                "couldnt": "couldn't",
                "wouldnt": "wouldn't",
                "didnt": "didn't",
                "doesnt": "doesn't",
                "isnt": "isn't",
                "arent": "aren't",
                "wasnt": "wasn't",
                "werent": "weren't",
                "havent": "haven't",
                "hasnt": "hasn't",
                "hadnt": "hadn't",
                "its": "it's",
                "thats": "that's",
                "heres": "here's",
                "theres": "there's",
                "wheres": "where's"
            }

            corrected_text = text

            # Apply corrections (case-insensitive)
            for wrong, correct in english_corrections.items():
                # Use regex for word boundaries to avoid partial replacements
                import re
                pattern = r'\b' + re.escape(wrong) + r'\b'
                corrected_text = re.sub(pattern, correct, corrected_text, flags=re.IGNORECASE)

            # Capitalize first letter of sentences
            sentences = corrected_text.split('. ')
            capitalized_sentences = []

            for sentence in sentences:
                if sentence.strip():
                    sentence = sentence.strip()
                    if sentence and sentence[0].islower():
                        sentence = sentence[0].upper() + sentence[1:]
                    capitalized_sentences.append(sentence)

            result = '. '.join(capitalized_sentences)

            # Ensure result ends with proper punctuation if original had it
            if text.endswith('.') and not result.endswith('.'):
                result += '.'

            return result

        except Exception as e:
            # DISABLED: English-specific corrections error logging
            return text

    def _error_result(self, error_msg: str) -> Dict[str, Any]:
        """Táº¡o error result"""
        return {
            'success': False,
            'error': error_msg,
            'transcript': '',
            'confidence': 0.0,
            'vietnamese_confidence': 0.0,
            'segments': 0,
            'duration': 0.0,
            'model': 'error',
            'language': 'unknown',
            'processing_time': 0.0,
            'whisper_time': 0.0,
            'gpt4o_time': 0.0,
            'original_text': '',
            'improved_text': ''
        }
    
    def _transcribe_with_whisper_only(self, audio_path: str, language: str = 'vi') -> Dict[str, Any]:
        """Transcribe chá»‰ sá»­ dá»¥ng OpenAI Whisper, KHÃ”NG cÃ³ GPT-4o improvement"""
        try:
            start_time = time.time()
            
            # Validate audio file
            if not os.path.exists(audio_path):
                return self._error_result("Audio file not found")
            
            # Get audio duration
            try:
                import librosa
                duration = librosa.get_duration(path=audio_path)
            except:
                duration = 0.0
            
            # Process audio
            processed_path = self.audio_processor.preprocess_audio(audio_path, self.config)
            if not processed_path:
                return self._error_result("Failed to process audio file")
            
            # Transcribe with OpenAI Whisper
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                return self._error_result("OpenAI API key not configured")
            
            from openai import OpenAI
            client = OpenAI(api_key=openai_api_key)
            
            whisper_start = time.time()
            
            with open(processed_path, "rb") as audio_file:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language=language,
                    response_format="verbose_json",
                    timestamp_granularities=["word"]
                )
            
            whisper_time = time.time()
            whisper_processing_time = whisper_time - whisper_start
            
            # Extract text and confidence
            text = transcript.text
            confidence = 0.9  # Default high confidence for OpenAI Whisper
            if hasattr(transcript, 'segments') and transcript.segments:
                total_confidence = sum(seg.get('avg_logprob', 0) for seg in transcript.segments)
                confidence = max(0.5, min(1.0, total_confidence / len(transcript.segments) + 0.5))
            
            total_processing_time = time.time() - start_time
            
            logger.info(f"ğŸµ Whisper-only transcription completed in {whisper_processing_time:.2f}s")
            logger.info(f"ğŸ“ Raw transcript: {text}")
            logger.info(f"ğŸ¯ Confidence: {confidence:.2f}")
            
            # Apply enhanced language model corrections (no GPT-4o)
            if language == 'vi':
                # DISABLED: Enhanced Vietnamese corrections logging
                corrected_text = self.language_model.correct_transcript(text)
                corrected_text = self._apply_vietnamese_specific_corrections(corrected_text)
                vietnamese_confidence = self.language_model.calculate_vietnamese_confidence(corrected_text)
                vietnamese_confidence = max(vietnamese_confidence, 0.85)  # Boost confidence for Vietnamese
                # DISABLED: Vietnamese corrections completed logging
            else:
                # DISABLED: Enhanced English corrections logging
                corrected_text = self._apply_english_specific_corrections(text)
                vietnamese_confidence = confidence
                # DISABLED: English corrections completed logging
            
            return {
                'success': True,
                'transcript': corrected_text,
                'confidence': confidence,
                'vietnamese_confidence': vietnamese_confidence,
                'segments': 1,
                'duration': duration,
                'model': 'openai-whisper-1-only',
                'language': language,
                'processing_time': total_processing_time,
                'whisper_time': whisper_processing_time,
                'gpt4o_time': 0.0,
                'original_text': text,
                'improved_text': text  # No improvement
            }
            
        except Exception as e:
            logger.error(f"âŒ Whisper-only transcription failed: {e}")
            return self._error_result(f"Whisper-only transcription failed: {e}")
    
    def get_system_info(self) -> Dict[str, Any]:
        """Láº¥y thÃ´ng tin system"""
        return {
            'model': getattr(self, 'model_name', 'Not loaded'),
            'is_initialized': self.is_initialized,
            'audio_processor_ready': self.audio_processor.is_initialized,
            'vocabulary_size': len(self.language_model.vietnamese_words),
            'config': {
                'chunk_duration': self.config.chunk_duration,
                'overlap_duration': self.config.overlap_duration,
                'min_confidence': self.config.min_confidence,
                'use_vad': self.config.use_vad,
                'denoise_audio': self.config.denoise_audio
            }
        }

# API cho Flask/FastAPI
class TranscriberAPI:
    """API wrapper cho transcriber"""
    
    def __init__(self):
        self.transcriber = None
        self._initialize()
    
    def _initialize(self):
        """Khá»Ÿi táº¡o transcriber"""
        try:
            config = TranscriptionConfig(
                chunk_duration=5.0,
                overlap_duration=1.0,
                min_confidence=0.7,
                use_vad=True,
                denoise_audio=True
            )
            
            self.transcriber = RealTimeVietnameseTranscriber(config)
            logger.info("âœ… Transcriber API initialized")
            
        except Exception as e:
            logger.error(f"âŒ API initialization failed: {e}")
    
    async def transcribe_file(self, file_path: str) -> Dict[str, Any]:
        """API endpoint Ä‘á»ƒ transcribe file"""
        if not self.transcriber or not self.transcriber.is_initialized:
            return {'error': 'Transcriber not ready', 'success': False}
        
        return self.transcriber.transcribe_audio_file(file_path)
    
    async def transcribe_audio_data(self, audio_data: bytes) -> Dict[str, Any]:
        """API endpoint Ä‘á»ƒ transcribe audio data"""
        try:
            # Save audio data to temporary file
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
                tmp.write(audio_data)
                tmp_path = tmp.name
            
            # Transcribe
            result = await self.transcribe_file(tmp_path)
            
            # Clean up
            os.unlink(tmp_path)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Audio data transcription failed: {e}")
            return {'error': str(e), 'success': False}
    
    def get_status(self) -> Dict[str, Any]:
        """Láº¥y tráº¡ng thÃ¡i API"""
        if self.transcriber:
            return self.transcriber.get_system_info()
        else:
            return {'status': 'not_initialized'}

# Test functions
def test_transcriber():
    """Test transcriber vá»›i file audio máº«u"""
    try:
        # Khá»Ÿi táº¡o
        config = TranscriptionConfig(
            chunk_duration=3.0,
            overlap_duration=0.5,
            min_confidence=0.6
        )
        
        transcriber = RealTimeVietnameseTranscriber(config)
        
        # Test vá»›i file audio
        test_file = "../frontend/test.mp3"
        if os.path.exists(test_file):
            result = transcriber.transcribe_audio_file(test_file)
            print("ğŸ¯ Test Result:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            print("âŒ Test file not found")
        
        # System info
        info = transcriber.get_system_info()
        print("\nğŸ”§ System Info:")
        print(json.dumps(info, indent=2, ensure_ascii=False))
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")

if __name__ == "__main__":
    test_transcriber()

# Flask API example
def create_flask_app():
    """Táº¡o Flask app cho transcription API"""
    try:
        from flask import Flask, request, jsonify, send_from_directory
        from werkzeug.utils import secure_filename
        import uuid
        
        app = Flask(__name__)
        app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size
        app.config['UPLOAD_FOLDER'] = 'uploads'
        
        # Táº¡o upload folder
        os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
        
        # Initialize API
        api = TranscriberAPI()
        
        @app.route('/api/health', methods=['GET'])
        def health_check():
            """Health check endpoint"""
            return jsonify(api.get_status())
        
        @app.route('/api/transcribe', methods=['POST'])
        def transcribe_endpoint():
            """Main transcription endpoint"""
            try:
                if 'audio' not in request.files:
                    return jsonify({'error': 'No audio file provided', 'success': False}), 400
                
                file = request.files['audio']
                if file.filename == '':
                    return jsonify({'error': 'No file selected', 'success': False}), 400
                
                # Save uploaded file
                filename = secure_filename(f"{uuid.uuid4()}_{file.filename}")
                filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                file.save(filepath)
                
                try:
                    # Transcribe
                    result = asyncio.run(api.transcribe_file(filepath))
                    
                    # Clean up
                    if os.path.exists(filepath):
                        os.unlink(filepath)
                    
                    return jsonify(result)
                    
                except Exception as e:
                    # Clean up on error
                    if os.path.exists(filepath):
                        os.unlink(filepath)
                    raise e
                
            except Exception as e:
                logger.error(f"âŒ Transcription endpoint error: {e}")
                return jsonify({'error': str(e), 'success': False}), 500
        
        @app.route('/api/transcribe/stream', methods=['POST'])
        def transcribe_stream():
            """Stream transcription endpoint"""
            try:
                audio_data = request.get_data()
                if not audio_data:
                    return jsonify({'error': 'No audio data provided', 'success': False}), 400
                
                result = asyncio.run(api.transcribe_audio_data(audio_data))
                return jsonify(result)
                
            except Exception as e:
                logger.error(f"âŒ Stream transcription error: {e}")
                return jsonify({'error': str(e), 'success': False}), 500
        
        @app.route('/api/models/info', methods=['GET'])
        def model_info():
            """Get model information"""
            return jsonify(api.get_status())
        
        return app
        
    except ImportError:
        logger.error("âŒ Flask not installed. Install with: pip install flask")
        return None

# FastAPI example
def create_fastapi_app():
    """Táº¡o FastAPI app cho transcription API"""
    try:
        from fastapi import FastAPI, UploadFile, File, HTTPException
        from fastapi.responses import JSONResponse
        from fastapi.middleware.cors import CORSMiddleware
        import uuid
        
        app = FastAPI(
            title="Vietnamese Speech Transcriber",
            description="Real-time Vietnamese speech-to-text API",
            version="2.0.0"
        )
        
        # CORS middleware
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        # Initialize API
        api = TranscriberAPI()
        
        @app.get("/api/health")
        async def health_check():
            """Health check endpoint"""
            return api.get_status()
        
        @app.post("/api/transcribe")
        async def transcribe_file_endpoint(audio: UploadFile = File(...)):
            """Main file transcription endpoint"""
            try:
                # Validate file type
                if not audio.content_type.startswith('audio/'):
                    raise HTTPException(status_code=400, detail="Invalid audio file")
                
                # Save uploaded file
                filename = f"{uuid.uuid4()}_{audio.filename}"
                filepath = f"uploads/{filename}"
                
                os.makedirs("uploads", exist_ok=True)
                
                with open(filepath, "wb") as f:
                    content = await audio.read()
                    f.write(content)
                
                try:
                    # Transcribe
                    result = await api.transcribe_file(filepath)
                    
                    # Clean up
                    if os.path.exists(filepath):
                        os.unlink(filepath)
                    
                    return result
                    
                except Exception as e:
                    # Clean up on error
                    if os.path.exists(filepath):
                        os.unlink(filepath)
                    raise HTTPException(status_code=500, detail=str(e))
                
            except Exception as e:
                logger.error(f"âŒ FastAPI transcription error: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @app.post("/api/transcribe/stream")
        async def transcribe_stream_endpoint(audio_data: bytes):
            """Stream transcription endpoint"""
            try:
                result = await api.transcribe_audio_data(audio_data)
                return result
                
            except Exception as e:
                logger.error(f"âŒ Stream transcription error: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        
        @app.get("/api/models/info")
        async def get_model_info():
            """Get model information"""
            return api.get_status()
        
        return app
        
    except ImportError:
        logger.error("âŒ FastAPI not installed. Install with: pip install fastapi uvicorn")
        return None

# Real-time WebSocket support
class WebSocketTranscriber:
    """WebSocket support cho real-time transcription"""
    
    def __init__(self):
        self.transcriber = None
        self.clients = set()
        self._initialize()
    
    def _initialize(self):
        """Khá»Ÿi táº¡o transcriber vá»›i real-time config"""
        config = TranscriptionConfig(
            chunk_duration=2.0,  # Shorter chunks for real-time
            overlap_duration=0.3,
            min_confidence=0.5,  # Lower threshold for real-time
            use_vad=True,
            denoise_audio=True,
            real_time_callback=self._broadcast_result
        )
        
        self.transcriber = RealTimeVietnameseTranscriber(config)
        self.transcriber.start_real_time_transcription()
    
    def _broadcast_result(self, result: Dict[str, Any], timestamp: float):
        """Broadcast káº¿t quáº£ Ä‘áº¿n táº¥t cáº£ clients"""
        message = {
            'type': 'transcription',
            'data': result,
            'timestamp': timestamp
        }
        
        # Remove disconnected clients
        disconnected = set()
        for client in self.clients:
            try:
                asyncio.create_task(client.send_json(message))
            except Exception:
                disconnected.add(client)
        
        self.clients -= disconnected
    
    def add_client(self, websocket):
        """ThÃªm WebSocket client"""
        self.clients.add(websocket)
    
    def remove_client(self, websocket):
        """XÃ³a WebSocket client"""
        self.clients.discard(websocket)
    
    def process_audio_chunk(self, audio_data: bytes):
        """Xá»­ lÃ½ audio chunk tá»« WebSocket"""
        if self.transcriber:
            self.transcriber.add_audio_chunk(audio_data)

# Usage examples and utilities
class TranscriptionBatch:
    """Batch processing multiple audio files"""
    
    def __init__(self, transcriber: RealTimeVietnameseTranscriber):
        self.transcriber = transcriber
    
    def process_directory(self, directory_path: str, output_file: str = None) -> List[Dict]:
        """Process táº¥t cáº£ audio files trong directory"""
        results = []
        
        audio_extensions = {'.mp3', '.wav', '.m4a', '.flac', '.ogg', '.aac'}
        directory = Path(directory_path)
        
        if not directory.exists():
            logger.error(f"âŒ Directory not found: {directory_path}")
            return results
        
        audio_files = [f for f in directory.iterdir() 
                      if f.suffix.lower() in audio_extensions]
        
        logger.info(f"ğŸ“ Processing {len(audio_files)} audio files...")
        
        for audio_file in audio_files:
            logger.info(f"ğŸµ Processing: {audio_file.name}")
            
            result = self.transcriber.transcribe_audio_file(str(audio_file))
            result['filename'] = audio_file.name
            results.append(result)
        
        # Save results if output file specified
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ Results saved to: {output_file}")
        
        return results
    
    def generate_report(self, results: List[Dict]) -> Dict[str, Any]:
        """Táº¡o bÃ¡o cÃ¡o tá»•ng káº¿t"""
        if not results:
            return {'error': 'No results to analyze'}
        
        successful = [r for r in results if r.get('success', False)]
        total_files = len(results)
        success_rate = len(successful) / total_files if total_files > 0 else 0
        
        # Calculate statistics
        confidences = [r.get('confidence', 0) for r in successful]
        vietnamese_confidences = [r.get('vietnamese_confidence', 0) for r in successful]
        durations = [r.get('duration', 0) for r in successful]
        
        report = {
            'summary': {
                'total_files': total_files,
                'successful_transcriptions': len(successful),
                'success_rate': round(success_rate, 3),
                'total_duration': round(sum(durations), 2) if durations else 0
            },
            'quality_metrics': {
                'avg_confidence': round(np.mean(confidences), 3) if confidences else 0,
                'min_confidence': round(np.min(confidences), 3) if confidences else 0,
                'max_confidence': round(np.max(confidences), 3) if confidences else 0,
                'avg_vietnamese_confidence': round(np.mean(vietnamese_confidences), 3) if vietnamese_confidences else 0
            },
            'performance': {
                'avg_duration': round(np.mean(durations), 2) if durations else 0,
                'total_processing_time': round(sum(durations), 2) if durations else 0
            }
        }
        
        return report

# Backward compatibility: expose expected class name
VietnameseTranscriber = RealTimeVietnameseTranscriber

# Configuration management
class TranscriberConfig:
    """Quáº£n lÃ½ cáº¥u hÃ¬nh cho transcriber"""
    
    @staticmethod
    def create_high_accuracy_config() -> TranscriptionConfig:
        """Cáº¥u hÃ¬nh cho Ä‘á»™ chÃ­nh xÃ¡c cao"""
        return TranscriptionConfig(
            chunk_duration=5.0,
            overlap_duration=1.0,
            min_confidence=0.8,
            use_vad=True,
            denoise_audio=True,
            language_detection=True
        )
    
    @staticmethod
    def create_real_time_config() -> TranscriptionConfig:
        """Cáº¥u hÃ¬nh cho real-time processing"""
        return TranscriptionConfig(
            chunk_duration=2.0,
            overlap_duration=0.5,
            min_confidence=0.6,
            use_vad=True,
            denoise_audio=False,  # Skip for speed
            language_detection=False,
            max_workers=1
        )
    
    @staticmethod
    def create_fast_config() -> TranscriptionConfig:
        """Cáº¥u hÃ¬nh cho xá»­ lÃ½ nhanh"""
        return TranscriptionConfig(
            chunk_duration=3.0,
            overlap_duration=0.3,
            min_confidence=0.5,
            use_vad=False,
            denoise_audio=False,
            language_detection=False,
            max_workers=4
        )

# Main execution
def main():
    """Main function vá»›i command line interface"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Vietnamese Speech Transcriber')
    parser.add_argument('--mode', choices=['test', 'flask', 'fastapi', 'batch'], 
                       default='test', help='Running mode')
    parser.add_argument('--file', type=str, help='Audio file to transcribe')
    parser.add_argument('--directory', type=str, help='Directory containing audio files')
    parser.add_argument('--output', type=str, help='Output file for results')
    parser.add_argument('--config', choices=['accuracy', 'realtime', 'fast'], 
                       default='accuracy', help='Configuration preset')
    parser.add_argument('--host', type=str, default='0.0.0.0', help='API host')
    parser.add_argument('--port', type=int, default=8000, help='API port')
    
    args = parser.parse_args()
    
    # Select configuration
    config_map = {
        'accuracy': TranscriberConfig.create_high_accuracy_config(),
        'realtime': TranscriberConfig.create_real_time_config(),
        'fast': TranscriberConfig.create_fast_config()
    }
    config = config_map[args.config]
    
    if args.mode == 'test':
        if args.file:
            # Test specific file
            transcriber = RealTimeVietnameseTranscriber(config)
            result = transcriber.transcribe_audio_file(args.file)
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            # Run default test
            test_transcriber()
    
    elif args.mode == 'batch':
        if not args.directory:
            print("âŒ --directory required for batch mode")
            return
        
        transcriber = RealTimeVietnameseTranscriber(config)
        batch_processor = TranscriptionBatch(transcriber)
        
        results = batch_processor.process_directory(args.directory, args.output)
        report = batch_processor.generate_report(results)
        
        print("ğŸ“Š Batch Processing Report:")
        print(json.dumps(report, indent=2, ensure_ascii=False))
    
    elif args.mode == 'flask':
        app = create_flask_app()
        if app:
            print(f"ğŸš€ Starting Flask server on {args.host}:{args.port}")
            app.run(host=args.host, port=args.port, debug=False)
    
    elif args.mode == 'fastapi':
        app = create_fastapi_app()
        if app:
            try:
                import uvicorn
                print(f"ğŸš€ Starting FastAPI server on {args.host}:{args.port}")
                uvicorn.run(app, host=args.host, port=args.port)
            except ImportError:
                print("âŒ uvicorn not installed. Install with: pip install uvicorn")

if __name__ == "__main__":
    main()