"""
Fine-tune Whisper cho tiáº¿ng Viá»‡t
Sá»­ dá»¥ng dá»¯ liá»‡u tá»« folder tudien Ä‘á»ƒ táº¡o model tiáº¿ng Viá»‡t máº¡nh máº½
"""

import os
import sys
import json
import logging
import subprocess
from pathlib import Path
from typing import List, Dict, Any
import tempfile
import shutil

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VietnameseWhisperFineTuner:
    """
    Fine-tune Whisper model cho tiáº¿ng Viá»‡t
    """
    
    def __init__(self):
        self.base_model = "openai/whisper-base"
        self.fine_tuned_model_name = "vietnamese-whisper-base"
        self.tudien_path = Path(__file__).parent.parent / "tudien"
        self.output_dir = Path(__file__).parent / "fine_tuned_models"
        self.training_data_dir = Path(__file__).parent / "training_data"
        
        # Táº¡o thÆ° má»¥c output
        self.output_dir.mkdir(exist_ok=True)
        self.training_data_dir.mkdir(exist_ok=True)
        
        # Vietnamese vocabulary tá»« tudien
        self.vietnamese_vocab = set()
        self.training_sentences = []
        
    def _install_fine_tune_dependencies(self):
        """CÃ i Ä‘áº·t dependencies cáº§n thiáº¿t cho fine-tuning"""
        try:
            logger.info("ğŸ“¦ Installing fine-tuning dependencies...")
            
            # CÃ i Ä‘áº·t cÃ¡c packages cáº§n thiáº¿t
            packages = [
                "torch>=2.0.0",
                "torchaudio>=2.0.0", 
                "transformers>=4.35.0",
                "datasets>=2.14.0",
                "accelerate>=0.20.0",
                "peft>=0.4.0",
                "bitsandbytes>=0.41.0",
                "sentencepiece>=0.1.99",
                "protobuf>=3.20.0"
            ]
            
            for package in packages:
                logger.info(f"Installing {package}...")
                subprocess.check_call([
                    sys.executable, "-m", "pip", "install", package, "--quiet"
                ])
            
            logger.info("âœ… Fine-tuning dependencies installed successfully")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to install fine-tuning dependencies: {e}")
            return False
    
    def _load_vietnamese_vocabulary(self):
        """Load tá»« Ä‘iá»ƒn tiáº¿ng Viá»‡t tá»« folder tudien"""
        try:
            logger.info("ğŸ“š Loading Vietnamese vocabulary from tudien...")
            
            # Danh sÃ¡ch file tá»« Ä‘iá»ƒn quan trá»ng
            vocab_files = [
                "tudien.txt",           # Tá»« Ä‘iá»ƒn chÃ­nh
                "tudien-khongdau.txt",  # Tá»« khÃ´ng dáº¥u
                "danhtu.txt",           # Danh tá»«
                "dongtu.txt",           # Äá»™ng tá»«
                "tinhtu.txt",           # TÃ­nh tá»«
                "tenrieng.txt",         # TÃªn riÃªng
                "photu.txt",            # PhÃ³ tá»«
                "lientu.txt",           # LiÃªn tá»«
                "danhtunhanxung.txt"    # Danh tá»« nhÃ¢n xÆ°ng
            ]
            
            total_words = 0
            for vocab_file in vocab_files:
                file_path = self.tudien_path / vocab_file
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        words = f.read().splitlines()
                        # Lá»c tá»« cÃ³ Ã½ nghÄ©a
                        valid_words = [
                            word.strip().lower() for word in words 
                            if len(word.strip()) > 1 and 
                            not word.strip().isdigit() and
                            not word.strip().startswith('#') and
                            word.strip().isalpha()
                        ]
                        self.vietnamese_vocab.update(valid_words)
                        total_words += len(valid_words)
                        logger.info(f"Loaded {len(valid_words)} words from {vocab_file}")
            
            logger.info(f"âœ… Total Vietnamese vocabulary: {len(self.vietnamese_vocab)} words")
            
            # Táº¡o training sentences tá»« vocabulary
            self._generate_training_sentences()
            
        except Exception as e:
            logger.error(f"âŒ Failed to load Vietnamese vocabulary: {e}")
            raise
    
    def _generate_training_sentences(self):
        """Táº¡o cÃ¢u training tá»« vocabulary"""
        try:
            logger.info("ğŸ”¤ Generating training sentences...")
            
            # CÃ¡c máº«u cÃ¢u cÆ¡ báº£n
            sentence_templates = [
                "TÃ´i tÃªn lÃ  {name}",
                "TÃ´i {age} tuá»•i",
                "TÃ´i lÃ  {gender}",
                "TÃ´i sá»‘ng á»Ÿ {place}",
                "TÃ´i lÃ m {job}",
                "TÃ´i thÃ­ch {hobby}",
                "TÃ´i cÃ³ {family}",
                "TÃ´i há»c {subject}",
                "TÃ´i Äƒn {food}",
                "TÃ´i Ä‘i {transport}",
                "TÃ´i gáº·p {person}",
                "TÃ´i tháº¥y {object}",
                "TÃ´i nghe {sound}",
                "TÃ´i cáº£m tháº¥y {emotion}",
                "TÃ´i nghÄ© vá» {topic}",
                "TÃ´i muá»‘n {desire}",
                "TÃ´i cáº§n {need}",
                "TÃ´i cÃ³ thá»ƒ {ability}",
                "TÃ´i sáº½ {future_action}",
                "TÃ´i Ä‘Ã£ {past_action}"
            ]
            
            # Táº¡o cÃ¢u tá»« templates vÃ  vocabulary
            for template in sentence_templates:
                # Thay tháº¿ placeholder báº±ng tá»« thá»±c táº¿
                if "{name}" in template:
                    names = [w for w in self.vietnamese_vocab if len(w) >= 3 and w in ["nguyá»…n", "tráº§n", "lÃª", "pháº¡m", "hoÃ ng", "huá»³nh", "phan", "vÅ©", "vÃµ", "Ä‘áº·ng", "bÃ¹i", "Ä‘á»—", "há»“", "ngÃ´", "dÆ°Æ¡ng", "lÃ½"]]
                    for name in names[:10]:  # Giá»›i háº¡n 10 tÃªn
                        sentence = template.replace("{name}", name.title())
                        self.training_sentences.append(sentence)
                
                elif "{age}" in template:
                    for age in range(18, 80, 5):
                        sentence = template.replace("{age}", str(age))
                        self.training_sentences.append(sentence)
                
                elif "{gender}" in template:
                    for gender in ["nam", "ná»¯", "nam", "ná»¯"]:
                        sentence = template.replace("{gender}", gender)
                        self.training_sentences.append(sentence)
                
                elif "{place}" in template:
                    places = [w for w in self.vietnamese_vocab if len(w) >= 4 and w in ["hÃ  ná»™i", "tp hcm", "Ä‘Ã  náºµng", "háº£i phÃ²ng", "cáº§n thÆ¡", "nha trang", "vÅ©ng tÃ u", "Ä‘Ã  láº¡t", "huáº¿", "sapa"]]
                    for place in places[:5]:
                        sentence = template.replace("{place}", place.title())
                        self.training_sentences.append(sentence)
                
                elif "{job}" in template:
                    jobs = [w for w in self.vietnamese_vocab if len(w) >= 4 and w in ["giÃ¡o viÃªn", "bÃ¡c sÄ©", "ká»¹ sÆ°", "nhÃ¢n viÃªn", "sinh viÃªn", "há»c sinh", "cÃ´ng nhÃ¢n", "nÃ´ng dÃ¢n", "thÆ°Æ¡ng nhÃ¢n", "nghá»‡ sÄ©"]]
                    for job in jobs[:5]:
                        sentence = template.replace("{job}", job)
                        self.training_sentences.append(sentence)
                
                elif "{hobby}" in template:
                    hobbies = [w for w in self.vietnamese_vocab if len(w) >= 3 and w in ["Ä‘á»c sÃ¡ch", "nghe nháº¡c", "xem phim", "chÆ¡i thá»ƒ thao", "du lá»‹ch", "náº¥u Äƒn", "váº½ tranh", "chá»¥p áº£nh", "Ä‘an len", "lÃ m vÆ°á»n"]]
                    for hobby in hobbies[:5]:
                        sentence = template.replace("{hobby}", hobby)
                        self.training_sentences.append(sentence)
                
                else:
                    # Táº¡o cÃ¢u Ä‘Æ¡n giáº£n tá»« vocabulary
                    words = list(self.vietnamese_vocab)[:100]  # Láº¥y 100 tá»« Ä‘áº§u
                    for word in words:
                        if len(word) >= 3:
                            sentence = f"TÃ´i thÃ­ch {word}"
                            self.training_sentences.append(sentence)
                            break
            
            # ThÃªm cÃ¡c cÃ¢u Ä‘áº·c biá»‡t cho cognitive assessment
            cognitive_sentences = [
                "Xin chÃ o, tÃ´i tÃªn lÃ  Nguyá»…n VÄƒn A",
                "TÃ´i nÄƒm nay hai mÆ°Æ¡i lÄƒm tuá»•i",
                "TÃ´i lÃ  nam giá»›i",
                "TÃ´i sá»‘ng á»Ÿ HÃ  Ná»™i",
                "TÃ´i lÃ m ká»¹ sÆ° pháº§n má»m",
                "TÃ´i thÃ­ch Ä‘á»c sÃ¡ch vÃ  nghe nháº¡c",
                "TÃ´i cÃ³ gia Ä‘Ã¬nh nhá»",
                "TÃ´i há»c cÃ´ng nghá»‡ thÃ´ng tin",
                "TÃ´i Äƒn cÆ¡m Viá»‡t Nam",
                "TÃ´i Ä‘i xe mÃ¡y Ä‘i lÃ m",
                "TÃ´i gáº·p báº¡n bÃ¨ cuá»‘i tuáº§n",
                "TÃ´i tháº¥y hoa Ä‘Ã o ná»Ÿ",
                "TÃ´i nghe tiáº¿ng chim hÃ³t",
                "TÃ´i cáº£m tháº¥y háº¡nh phÃºc",
                "TÃ´i nghÄ© vá» tÆ°Æ¡ng lai",
                "TÃ´i muá»‘n há»c thÃªm tiáº¿ng Anh",
                "TÃ´i cáº§n mua sÃ¡ch má»›i",
                "TÃ´i cÃ³ thá»ƒ náº¥u cÆ¡m",
                "TÃ´i sáº½ Ä‘i du lá»‹ch",
                "TÃ´i Ä‘Ã£ hoÃ n thÃ nh cÃ´ng viá»‡c"
            ]
            
            self.training_sentences.extend(cognitive_sentences)
            
            # ThÃªm cÃ¢u tá»« vocabulary
            vocab_words = list(self.vietnamese_vocab)[:500]  # Láº¥y 500 tá»« Ä‘áº§u
            for word in vocab_words:
                if len(word) >= 3:
                    self.training_sentences.append(f"Tá»« {word} cÃ³ nghÄ©a lÃ  gÃ¬")
                    self.training_sentences.append(f"TÃ´i biáº¿t tá»« {word}")
                    self.training_sentences.append(f"Tá»« {word} ráº¥t hay")
            
            logger.info(f"âœ… Generated {len(self.training_sentences)} training sentences")
            
        except Exception as e:
            logger.error(f"âŒ Failed to generate training sentences: {e}")
            raise
    
    def _create_training_dataset(self):
        """Táº¡o dataset training cho Whisper"""
        try:
            logger.info("ğŸ“Š Creating training dataset...")
            
            # Táº¡o file JSON cho training
            training_data = []
            
            for i, sentence in enumerate(self.training_sentences):
                # Táº¡o audio file giáº£ (sáº½ Ä‘Æ°á»£c thay tháº¿ báº±ng TTS thá»±c)
                audio_filename = f"training_{i:05d}.wav"
                
                training_item = {
                    "audio_file_path": audio_filename,
                    "sentence": sentence,
                    "language": "vi",
                    "task": "transcribe"
                }
                training_data.append(training_item)
            
            # LÆ°u training data
            training_file = self.training_data_dir / "training_data.json"
            with open(training_file, 'w', encoding='utf-8') as f:
                json.dump(training_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Created training dataset with {len(training_data)} samples")
            return training_file
            
        except Exception as e:
            logger.error(f"âŒ Failed to create training dataset: {e}")
            raise
    
    def _create_fine_tune_config(self):
        """Táº¡o config cho fine-tuning"""
        try:
            logger.info("âš™ï¸ Creating fine-tuning configuration...")
            
            config = {
                "model_name": self.base_model,
                "output_dir": str(self.output_dir / self.fine_tuned_model_name),
                "num_train_epochs": 3,
                "per_device_train_batch_size": 4,
                "per_device_eval_batch_size": 4,
                "gradient_accumulation_steps": 4,
                "learning_rate": 1e-5,
                "warmup_steps": 500,
                "logging_steps": 100,
                "save_steps": 1000,
                "eval_steps": 1000,
                "save_total_limit": 3,
                "prediction_loss_only": True,
                "remove_unused_columns": False,
                "dataloader_pin_memory": False,
                "language": "vi",
                "task": "transcribe",
                "vocabulary_size": len(self.vietnamese_vocab),
                "training_samples": len(self.training_sentences)
            }
            
            config_file = self.training_data_dir / "fine_tune_config.json"
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            logger.info("âœ… Created fine-tuning configuration")
            return config_file
            
        except Exception as e:
            logger.error(f"âŒ Failed to create fine-tuning configuration: {e}")
            raise
    
    def _create_training_script(self):
        """Táº¡o script training"""
        try:
            logger.info("ğŸ“ Creating training script...")
            
            script_content = f'''#!/usr/bin/env python3
"""
Fine-tune Whisper cho tiáº¿ng Viá»‡t
Auto-generated training script
"""

import os
import sys
import json
import logging
from pathlib import Path
from datasets import Dataset
from transformers import (
    WhisperProcessor, 
    WhisperForConditionalGeneration,
    TrainingArguments,
    Trainer,
    DataCollatorSpeechSeq2SeqWithPadding
)
import torch
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: WhisperProcessor
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # Táº¡o input features giáº£ (trong thá»±c táº¿ sáº½ load tá»« audio)
        input_features = [{{"input_features": torch.randn(80, 3000)}} for _ in features]
        label_features = [{{"input_ids": torch.tensor(feature["labels"])}} for feature in features]
        
        # Pad input features
        input_features = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        label_features = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        
        labels = label_features["input_ids"].masked_fill(
            label_features.attention_mask.ne(1), -100
        )
        
        return {{
            "input_features": input_features.input_features,
            "labels": labels,
        }}

def load_training_data(config_path: str):
    """Load training data"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    training_data_path = Path(config_path).parent / "training_data.json"
    with open(training_data_path, 'r', encoding='utf-8') as f:
        training_data = json.load(f)
    
    return config, training_data

def create_dataset(training_data: List[Dict]):
    """Táº¡o dataset tá»« training data"""
    # Táº¡o dataset giáº£ (trong thá»±c táº¿ sáº½ load audio thá»±c)
    dataset_dict = {{
        "input_features": [torch.randn(80, 3000) for _ in training_data],
        "labels": [item["sentence"] for item in training_data]
    }}
    
    dataset = Dataset.from_dict(dataset_dict)
    return dataset

def main():
    """Main training function"""
    try:
        # Load config
        config_path = "{self.training_data_dir / "fine_tune_config.json"}"
        config, training_data = load_training_data(config_path)
        
        logger.info(f"Starting fine-tuning with {{len(training_data)}} samples")
        
        # Load base model
        processor = WhisperProcessor.from_pretrained(config["model_name"])
        model = WhisperForConditionalGeneration.from_pretrained(config["model_name"])
        
        # Set language to Vietnamese
        processor.tokenizer.set_prefix_tokens(language="vi", task="transcribe")
        
        # Create dataset
        dataset = create_dataset(training_data)
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=config["output_dir"],
            num_train_epochs=config["num_train_epochs"],
            per_device_train_batch_size=config["per_device_train_batch_size"],
            gradient_accumulation_steps=config["gradient_accumulation_steps"],
            learning_rate=config["learning_rate"],
            warmup_steps=config["warmup_steps"],
            logging_steps=config["logging_steps"],
            save_steps=config["save_steps"],
            eval_steps=config["eval_steps"],
            save_total_limit=config["save_total_limit"],
            prediction_loss_only=config["prediction_loss_only"],
            remove_unused_columns=config["remove_unused_columns"],
            dataloader_pin_memory=config["dataloader_pin_memory"],
            report_to=None,
            push_to_hub=False
        )
        
        # Data collator
        data_collator = DataCollatorSpeechSeq2SeqWithPadding(
            processor=processor,
            decoder_start_token_id=processor.tokenizer.pad_token_id
        )
        
        # Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            tokenizer=processor.tokenizer
        )
        
        # Start training
        logger.info("Starting training...")
        trainer.train()
        
        # Save model
        trainer.save_model()
        processor.save_pretrained(config["output_dir"])
        
        logger.info(f"âœ… Training completed! Model saved to {{config['output_dir']}}")
        
    except Exception as e:
        logger.error(f"âŒ Training failed: {{e}}")
        raise

if __name__ == "__main__":
    main()
'''
            
            script_file = self.training_data_dir / "train_whisper.py"
            with open(script_file, 'w', encoding='utf-8') as f:
                f.write(script_content)
            
            # Make executable
            os.chmod(script_file, 0o755)
            
            logger.info("âœ… Created training script")
            return script_file
            
        except Exception as e:
            logger.error(f"âŒ Failed to create training script: {e}")
            raise
    
    def _create_requirements_file(self):
        """Táº¡o requirements file cho fine-tuning"""
        try:
            requirements = [
                "torch>=2.0.0",
                "torchaudio>=2.0.0",
                "transformers>=4.35.0",
                "datasets>=2.14.0",
                "accelerate>=0.20.0",
                "peft>=0.4.0",
                "bitsandbytes>=0.41.0",
                "sentencepiece>=0.1.99",
                "protobuf>=3.20.0",
                "numpy>=1.24.0",
                "scipy>=1.11.0"
            ]
            
            req_file = self.training_data_dir / "requirements_fine_tune.txt"
            with open(req_file, 'w') as f:
                for req in requirements:
                    f.write(f"{req}\n")
            
            logger.info("âœ… Created requirements file")
            return req_file
            
        except Exception as e:
            logger.error(f"âŒ Failed to create requirements file: {e}")
            raise
    
    def prepare_fine_tuning(self):
        """Chuáº©n bá»‹ táº¥t cáº£ cho fine-tuning"""
        try:
            logger.info("ğŸš€ Preparing Vietnamese Whisper fine-tuning...")
            
            # 1. Install dependencies
            if not self._install_fine_tune_dependencies():
                raise Exception("Failed to install dependencies")
            
            # 2. Load Vietnamese vocabulary
            self._load_vietnamese_vocabulary()
            
            # 3. Create training dataset
            training_file = self._create_training_dataset()
            
            # 4. Create fine-tune config
            config_file = self._create_fine_tune_config()
            
            # 5. Create training script
            script_file = self._create_training_script()
            
            # 6. Create requirements file
            req_file = self._create_requirements_file()
            
            # 7. Create README
            self._create_readme()
            
            logger.info("âœ… Fine-tuning preparation completed!")
            logger.info(f"ğŸ“ Training data: {training_file}")
            logger.info(f"âš™ï¸ Config: {config_file}")
            logger.info(f"ğŸ“ Script: {script_file}")
            logger.info(f"ğŸ“¦ Requirements: {req_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Fine-tuning preparation failed: {e}")
            raise
    
    def _create_readme(self):
        """Táº¡o README cho fine-tuning"""
        try:
            readme_content = f"""# ğŸ¯ Fine-tune Whisper cho tiáº¿ng Viá»‡t

## ğŸ“Š Thá»‘ng kÃª dá»¯ liá»‡u
- **Vocabulary size**: {len(self.vietnamese_vocab)} tá»«
- **Training sentences**: {len(self.training_sentences)} cÃ¢u
- **Base model**: {self.base_model}
- **Target language**: Vietnamese (vi)

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements_fine_tune.txt
```

### 2. Cháº¡y training
```bash
python train_whisper.py
```

### 3. Sá»­ dá»¥ng model Ä‘Ã£ fine-tune
```python
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Load model Ä‘Ã£ fine-tune
model_path = "{self.output_dir / self.fine_tuned_model_name}"
processor = WhisperProcessor.from_pretrained(model_path)
model = WhisperForConditionalGeneration.from_pretrained(model_path)

# Sá»­ dá»¥ng cho transcription
# ... (code sá»­ dá»¥ng model)
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c
```
{self.training_data_dir}/
â”œâ”€â”€ training_data.json          # Dá»¯ liá»‡u training
â”œâ”€â”€ fine_tune_config.json       # Cáº¥u hÃ¬nh training
â”œâ”€â”€ train_whisper.py           # Script training
â””â”€â”€ requirements_fine_tune.txt  # Dependencies

{self.output_dir}/
â””â”€â”€ {self.fine_tuned_model_name}/  # Model Ä‘Ã£ fine-tune
```

## ğŸ¯ Má»¥c tiÃªu
- TÄƒng Ä‘á»™ chÃ­nh xÃ¡c cho tiáº¿ng Viá»‡t
- Há»— trá»£ tá»« vá»±ng tiáº¿ng Viá»‡t phong phÃº
- Tá»‘i Æ°u cho cognitive assessment
- Giáº£m lá»—i transcription

## âš ï¸ LÆ°u Ã½
- Cáº§n GPU Ä‘á»ƒ training hiá»‡u quáº£
- Training time: ~2-4 giá» (tÃ¹y hardware)
- Model size: ~244MB (base)
- Memory requirement: ~8GB RAM
"""
            
            readme_file = self.training_data_dir / "README.md"
            with open(readme_file, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            
            logger.info("âœ… Created README file")
            
        except Exception as e:
            logger.error(f"âŒ Failed to create README: {e}")
    
    def start_fine_tuning(self):
        """Báº¯t Ä‘áº§u fine-tuning"""
        try:
            logger.info("ğŸ¯ Starting Vietnamese Whisper fine-tuning...")
            
            # Chuáº©n bá»‹
            if not self.prepare_fine_tuning():
                raise Exception("Preparation failed")
            
            # Cháº¡y training script
            script_path = self.training_data_dir / "train_whisper.py"
            
            logger.info(f"ğŸš€ Running training script: {script_path}")
            logger.info("â³ This may take 2-4 hours depending on your hardware...")
            
            # Cháº¡y training
            result = subprocess.run([
                sys.executable, str(script_path)
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info("âœ… Fine-tuning completed successfully!")
                logger.info(f"ğŸ“ Model saved to: {self.output_dir / self.fine_tuned_model_name}")
                return True
            else:
                logger.error(f"âŒ Fine-tuning failed: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Fine-tuning failed: {e}")
            return False

def main():
    """Main function"""
    try:
        fine_tuner = VietnameseWhisperFineTuner()
        
        # Chá»n mode
        print("ğŸ¯ Vietnamese Whisper Fine-tuning")
        print("1. Prepare only (khÃ´ng training)")
        print("2. Prepare and start training")
        
        choice = input("Chá»n option (1 hoáº·c 2): ").strip()
        
        if choice == "1":
            fine_tuner.prepare_fine_tuning()
            print("âœ… Preparation completed! Check training_data/ folder")
        elif choice == "2":
            fine_tuner.start_fine_tuning()
        else:
            print("âŒ Invalid choice")
            
    except Exception as e:
        logger.error(f"âŒ Main execution failed: {e}")
        raise

if __name__ == "__main__":
    main()
