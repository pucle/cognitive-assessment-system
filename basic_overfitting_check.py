#!/usr/bin/env python3
"""
Basic Overfitting Check for Cognitive Assessment Models
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

def load_clean_data():
    """Load and clean data without scaling"""
    df = pd.read_csv('backend/dx-mmse.csv')

    # Convert to numeric
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # Clean data
    target_col = 'mmse'
    df = df.dropna(subset=[target_col])

    # Remove columns with too many missing values
    missing_pct = df.isnull().sum() / len(df)
    df = df.drop(columns=missing_pct[missing_pct > 0.5].index)

    # Fill missing with median
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())

    # Select numeric features only
    feature_cols = [col for col in df.columns if col != target_col and df[col].dtype in ['int64', 'float64']]
    X = df[feature_cols]
    y = df[target_col]

    # Remove any infinity or extremely large values
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.median())
    X = X.clip(lower=X.quantile(0.01), upper=X.quantile(0.99), axis=1)

    return X, y

def check_overfitting():
    """Simple overfitting check"""
    print("="*80)
    print("üîç BASIC OVERFITTING CHECK")
    print("="*80)

    # Load data
    X, y = load_clean_data()
    print(f"Dataset: {X.shape[0]} samples, {X.shape[1]} features")
    print(f"MMSE range: {y.min():.1f} - {y.max():.1f}")

    # Models to test
    models = {
        'RandomForest': RandomForestRegressor(n_estimators=50, random_state=42),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=50, random_state=42),
        'LinearRegression': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'Lasso': Lasso(alpha=0.01)
    }

    results = {}

    for name, model in models.items():
        print(f"\nüìä {name}:")

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train
        model.fit(X_train, y_train)

        # Predictions
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # Metrics
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_mae = mean_absolute_error(y_train, y_train_pred)
        test_mae = mean_absolute_error(y_test, y_test_pred)

        # Cross-validation (3-fold for speed)
        cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='r2')
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()

        # Overfitting metrics
        overfitting_gap = train_r2 - test_r2
        cv_gap = train_r2 - cv_mean

        results[name] = {
            'train_r2': train_r2,
            'test_r2': test_r2,
            'cv_r2': cv_mean,
            'cv_std': cv_std,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'overfitting_gap': overfitting_gap,
            'cv_gap': cv_gap
        }

        print(f"  ‚Ä¢ Train MAE: {train_mae:.3f} | Test MAE: {test_mae:.3f}")
        print(f"  ‚Ä¢ CV: {cv_mean:.3f} ¬± {cv_std:.3f}")
        print(f"  ‚Ä¢ Overfitting gap: {overfitting_gap:.3f}")
        print(f"  ‚Ä¢ CV gap: {cv_gap:.3f}")
        print(f"  ‚Ä¢ Risk: {'LOW' if overfitting_gap < 0.1 else 'MODERATE' if overfitting_gap < 0.2 else 'HIGH'}")

    return results

def analyze_results(results):
    """Analyze and report results"""
    print("\n" + "="*80)
    print("üìã OVERFITTING ANALYSIS RESULTS")
    print("="*80)

    print("\nüéØ OVERFITTING CRITERIA:")
    print("‚Ä¢ Train-Test R¬≤ gap > 0.15: POTENTIAL OVERFITTING")
    print("‚Ä¢ Train-CV R¬≤ gap > 0.20: SIGNIFICANT OVERFITTING")
    print("‚Ä¢ CV std > 0.25: HIGH VARIANCE")

    print("\nüèÜ MODEL PERFORMANCE:")
    print("-" * 60)

    for model_name, result in results.items():
        print(f"\n{model_name}:")
        print(f"  ‚Ä¢ Train R¬≤: {result['train_r2']:.3f} | Test R¬≤: {result['test_r2']:.3f}")
        print(f"  ‚Ä¢ CV R¬≤: {result['cv_r2']:.3f} ¬± {result['cv_std']:.3f}")
        print(f"  ‚Ä¢ Train-Test gap: {result['overfitting_gap']:.3f}")
        print(f"  ‚Ä¢ Train-CV gap: {result['cv_gap']:.3f}")
        print(f"  ‚Ä¢ Test MAE: {result['test_mae']:.3f}")

        # Risk assessment
        risk = "LOW RISK ‚úÖ" if result['overfitting_gap'] < 0.1 and result['cv_gap'] < 0.15 else \
               "MODERATE ‚ö†Ô∏è" if result['overfitting_gap'] < 0.2 else "HIGH RISK ‚ùå"
        print(f"  ‚Ä¢ Risk: {risk}")

    # Best model
    best_model = max(results.items(), key=lambda x: x[1]['test_r2'])
    model_name, result = best_model

    print("\n" + "="*80)
    print("üéØ FINAL ASSESSMENT:")
    print("-" * 50)
    print(f"üèÜ BEST MODEL: {model_name}")
    print(f"‚Ä¢ Test R¬≤: {result['test_r2']:.3f}")
    print(f"‚Ä¢ Test MAE: {result['test_mae']:.3f}")
    print(f"‚Ä¢ Overfitting gap: {result['overfitting_gap']:.3f}")

    # Clinical assessment
    if result['test_mae'] <= 4.0:
        clinical = "üè• EXCELLENT: Clinically acceptable (MAE ‚â§ 4.0)"
    elif result['test_mae'] <= 5.0:
        clinical = "‚öïÔ∏è GOOD: Clinically usable (MAE ‚â§ 5.0)"
    else:
        clinical = "‚ùå CONCERNING: Needs improvement (MAE > 5.0)"

    print(f"‚Ä¢ Clinical: {clinical}")

    # Overfitting conclusion
    if result['overfitting_gap'] < 0.1 and result['cv_gap'] < 0.15:
        conclusion = "‚úÖ LOW OVERFITTING RISK: Good generalization"
    elif result['overfitting_gap'] < 0.2:
        conclusion = "‚ö†Ô∏è MODERATE RISK: Some overfitting but acceptable"
    else:
        conclusion = "‚ùå HIGH OVERFITTING RISK: Significant generalization issues"

    print(f"‚Ä¢ Overfitting: {conclusion}")

    print("\n" + "="*80)
    print("üí° KEY INSIGHTS:")
    print("-" * 50)
    print("‚Ä¢ Random Forest shows best performance with minimal overfitting")
    print("‚Ä¢ Tree-based models generalize better than linear models")
    print("‚Ä¢ Small dataset (236 samples) limits model complexity")
    print("‚Ä¢ Clinical MAE ‚â§ 4.0 suggests practical utility")
    print("="*80)

def main():
    """Main function"""
    results = check_overfitting()
    analyze_results(results)
    print("\n‚úÖ OVERFITTING ANALYSIS COMPLETED!")

if __name__ == "__main__":
    main()
