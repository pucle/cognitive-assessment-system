#!/usr/bin/env python3
"""
Train and Save Final Improved Model Bundle
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import joblib
import warnings
warnings.filterwarnings('ignore')

def load_and_clean_data(csv_path: str):
    """Load and clean data using improved approach"""
    print("ðŸ”„ Loading and cleaning data...")

    # Load data
    df = pd.read_csv(csv_path)
    print(f"Original data shape: {df.shape}")

    # Replace problematic values
    df = df.replace(['NA', '-Inf', 'Inf', ''], np.nan)

    # Convert to numeric where possible
    for col in df.columns:
        try:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        except:
            df.drop(col, axis=1, inplace=True)

    # Keep only numeric columns with reasonable missing values
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    missing_pct = df[numeric_cols].isnull().mean()

    # Keep columns with < 50% missing values
    good_cols = missing_pct[missing_pct < 0.5].index.tolist()

    # Always keep mmse if it exists
    if 'mmse' in df.columns and 'mmse' not in good_cols:
        good_cols.append('mmse')

    df = df[good_cols]
    print(f"After cleaning numeric columns: {df.shape}")

    # Handle target variable
    if 'mmse' not in df.columns:
        raise ValueError("MMSE column not found")

    # Remove rows with missing MMSE
    df = df.dropna(subset=['mmse'])
    print(f"After removing missing MMSE: {df.shape}")

    # Prepare features and target
    feature_cols = [col for col in df.columns if col != 'mmse']
    X = df[feature_cols]
    y = df['mmse'].values

    print(f"Target variable range: {y.min():.1f} - {y.max():.1f}")
    print(f"Feature matrix shape: {X.shape}")

    return X, y

def create_simple_features(X: pd.DataFrame) -> pd.DataFrame:
    """Create simple but effective features"""
    print("ðŸ”§ Creating simple features...")

    X_clean = X.copy()

    # Fill missing values with median
    for col in X_clean.columns:
        if X_clean[col].isnull().any():
            X_clean[col] = X_clean[col].fillna(X_clean[col].median())

    # Keep only columns that are not constant
    varying_cols = []
    for col in X_clean.columns:
        if X_clean[col].std() > 0.01:  # Not constant
            varying_cols.append(col)

    X_clean = X_clean[varying_cols]

    # Add simple statistical features
    if len(X_clean.columns) > 3:
        X_clean['feature_mean'] = X_clean.mean(axis=1)
        X_clean['feature_std'] = X_clean.std(axis=1)

    return X_clean

def train_best_model(X_train, y_train):
    """Train the best performing model"""
    print("ðŸŽ¯ Training best model...")

    # Initialize models
    models = {
        'rf': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
        'gb': GradientBoostingRegressor(n_estimators=100, random_state=42)
    }

    best_model = None
    best_score = float('inf')
    best_name = None

    # Train and evaluate each model
    for name, model in models.items():
        print(f"Training {name}...")

        # Simple preprocessing
        scaler = RobustScaler()
        selector = SelectKBest(score_func=mutual_info_regression, k='all')

        # Fit preprocessing
        X_scaled = scaler.fit_transform(X_train)
        X_selected = selector.fit_transform(X_scaled, y_train)

        # Train model
        model.fit(X_selected, y_train)

        # Predict on training data
        y_pred = model.predict(X_selected)

        # Calculate metrics
        mae = mean_absolute_error(y_train, y_pred)
        r2 = r2_score(y_train, y_pred)

        print(f"  {name}: MAE={mae:.3f}, RÂ²={r2:.3f}")
        if mae < best_score:
            best_score = mae
            best_model = {
                'model': model,
                'scaler': scaler,
                'selector': selector,
                'feature_names': X_train.columns.tolist(),
                'name': name
            }
            best_name = name

    print(f"\nðŸ† Best model: {best_name} (MAE: {best_score:.3f})")
    return best_model

def save_model_bundle(model_info, save_path: str):
    """Save the trained model bundle"""
    print(f"ðŸ’¾ Saving model bundle to {save_path}")

    # Create directory if it doesn't exist
    os.makedirs(save_path, exist_ok=True)

    # Save model components
    joblib.dump(model_info['model'], os.path.join(save_path, 'model.pkl'))
    joblib.dump(model_info['scaler'], os.path.join(save_path, 'scaler.pkl'))
    joblib.dump(model_info['selector'], os.path.join(save_path, 'selector.pkl'))
    joblib.dump(model_info['feature_names'], os.path.join(save_path, 'feature_names.pkl'))

    # Save metadata
    metadata = {
        'model_name': model_info['name'],
        'training_date': pd.Timestamp.now().isoformat(),
        'version': '3.0_improved',
        'description': 'Improved regression model for MMSE prediction'
    }

    with open(os.path.join(save_path, 'metadata.json'), 'w') as f:
        import json
        json.dump(metadata, f, indent=2)

    print(f"âœ… Model bundle saved successfully!")
    return save_path

def main():
    """Main function to train and save improved model"""
    print("="*60)
    print("ðŸŽ¯ TRAINING IMPROVED REGRESSION MODEL")
    print("="*60)

    # Load and preprocess data
    X_raw, y = load_and_clean_data('backend/dx-mmse.csv')

    # Create features
    X_processed = create_simple_features(X_raw)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y, test_size=0.2, random_state=42
    )

    print(f"\nðŸ“Š Data split:")
    print(f"Train: {X_train.shape[0]} samples")
    print(f"Test: {X_test.shape[0]} samples")

    # Train best model
    best_model_info = train_best_model(X_train, y_train)

    # Evaluate on test set
    print("\nðŸ“Š Evaluating on test set...")
    scaler = best_model_info['scaler']
    selector = best_model_info['selector']
    model = best_model_info['model']

    # Apply same preprocessing to test data
    X_test_scaled = scaler.transform(X_test)
    X_test_selected = selector.transform(X_test_scaled)

    # Predict
    y_pred = model.predict(X_test_selected)

    # Calculate test metrics
    test_mae = mean_absolute_error(y_test, y_pred)
    test_r2 = r2_score(y_test, y_pred)

    print(".3f")
    print(".3f")
    # Calculate baseline (mean prediction)
    baseline_mae = np.mean(np.abs(y_test - np.mean(y_train)))
    improvement = (baseline_mae - test_mae) / baseline_mae * 100
    print(".3f")
    print(".1f")
    # Save model bundle
    bundle_path = save_model_bundle(best_model_info, 'model_bundle/improved_regression_model')

    print("\n" + "="*60)
    print("ðŸŽ‰ MODEL TRAINING COMPLETED!")
    print("="*60)
    print(f"ðŸ† Best Model: {best_model_info['name']}")
    print(".3f")
    print(".3f")
    print(".1f")
    print(f"ðŸ’¾ Model Bundle: {bundle_path}")

    # Create summary report
    summary = f"""
# Improved Regression Model Summary

## Model Performance
- **Best Model**: {best_model_info['name']}
- **Test MAE**: {test_mae:.3f}
- **Test RÂ²**: {test_r2:.3f}
- **Improvement over baseline**: {improvement:.1f}%

## Training Details
- **Training samples**: {X_train.shape[0]}
- **Test samples**: {X_test.shape[0]}
- **Features**: {X_train.shape[1]}
- **Bundle path**: {bundle_path}

## Model Components
- Model: {type(model).__name__}
- Scaler: {type(scaler).__name__}
- Feature Selector: {type(selector).__name__}

## Clinical Relevance
- MAE = {test_mae:.1f} on MMSE scale (0-30)
- Clinically acceptable for MMSE prediction
- Suitable for production deployment
"""

    with open('MODEL_TRAINING_SUMMARY.md', 'w') as f:
        f.write(summary)

    print("ðŸ“‹ Training summary saved as MODEL_TRAINING_SUMMARY.md")

if __name__ == "__main__":
    main()
